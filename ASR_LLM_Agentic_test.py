import os
import io
import time
import re
import httpx
import json
import yt_dlp
import warnings
import concurrent.futures
import difflib
import random

from typing import List, Optional, Dict, Union
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser, StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from pydub import AudioSegment

# -------------------------------------------------
# ‡∏õ‡∏¥‡∏î Warning
# -------------------------------------------------
warnings.filterwarnings("ignore")

try:
    from duckduckgo_search import DDGS
except ImportError:
    DDGS = None

try:
    import yfinance as yf
except ImportError:
    yf = None

# ---------------- CONFIGURATION ----------------
TYPHOON_BASE_URL = "https://api.opentyphoon.ai/v1"
TYPHOON_API_KEY = "sk-vCE2QnUydpGnzic35kI3IcoTsAeWzb2X3jYCCAXDPmfT2JnN"

GOOGLE_BASE_URL = "https://generativelanguage.googleapis.com/v1beta/openai/"
GOOGLE_API_KEY = "AIzaSyCkjWUucxzLaRPnuklKxKgYP0fUyQhTwHA"
LLM_MODEL_NAME = "gemini-2.0-flash"

YOUTUBE_URL = "https://www.youtube.com/watch?v=opEIqiPzx64"

DOWNLOAD_DIR = "downloads"
TRANSCRIPT_OUTPUT_DIR = "transcripts_output"
RAW_TRANSCRIPT_FILE = "raw_transcript_full.txt"
CLEAN_TRANSCRIPT_FILE = "final_transcript_clean.txt"
MARKDOWN_FILE = "final_summary_markdown.md"

MASTER_KB_FILE = "knowledge_base.json"
CACHE_FILE = "ticker_cache.json"

CHUNK_DURATION_SEC = 45
OVERLAP_DURATION_SEC = 15
MAX_WORKERS = 5

# ---------------- PART 1: SMART CONTEXT MANAGER ----------------

class StockContextManager:
    def __init__(self, kb_file=MASTER_KB_FILE):
        self.kb_file = kb_file
        self.sector_data = {}
        self.flat_memory = {}
        self.load_kb()

    def load_kb(self):
        if os.path.exists(self.kb_file):
            try:
                with open(self.kb_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                if isinstance(data, dict):
                    self.sector_data = data
                    for sector, stocks in data.items():
                        if isinstance(stocks, dict):
                            for ticker, aliases in stocks.items():
                                for alias in aliases:
                                    self.flat_memory[alias.lower()] = ticker
                print(f"üìö Context Manager Loaded: {len(self.sector_data)} Sectors")
            except Exception as e:
                print(f"‚ö†Ô∏è Error loading KB: {e}")

    def get_sector_prompt_str(self):
        lines = []
        for sector, stocks in self.sector_data.items():
            if isinstance(stocks, dict):
                tickers = list(stocks.keys())
                clean_tickers = [t.replace('.BK', '') for t in tickers]
                lines.append(f"- {sector}: {', '.join(clean_tickers)}")
        return "\n".join(lines)

    def get_vocab_prompt(self):
        vocab_list = []
        for sector, stocks in self.sector_data.items():
            if isinstance(stocks, dict):
                for ticker, aliases in stocks.items():
                    clean_ticker = ticker.replace('.BK', '')
                    vocab_list.append(clean_ticker)
                    thai_aliases = [a for a in aliases if re.search(r'[‡∏Å-‡πô]', a)]
                    vocab_list.extend(thai_aliases)
        vocab_list.extend(["‡∏á‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô", "‡πÅ‡∏ô‡∏ß‡∏£‡∏±‡∏ö", "‡πÅ‡∏ô‡∏ß‡∏ï‡πâ‡∏≤‡∏ô", "‡∏î‡∏±‡∏ä‡∏ô‡∏µ", "SET", "SET50", "‡∏Å‡∏≥‡πÑ‡∏£", "‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™"])
        if len(vocab_list) > 100:
            selected_vocab = random.sample(vocab_list, 100)
        else:
            selected_vocab = vocab_list
        return f"‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô‡∏Ñ‡∏•‡∏¥‡∏õ: {', '.join(selected_vocab)}"

ctx_mgr = StockContextManager()
SECTOR_CONTEXT_STR = ctx_mgr.get_sector_prompt_str()
ASR_VOCAB_PROMPT = ctx_mgr.get_vocab_prompt()
print(f"üé§ ASR Vocab Hint: {ASR_VOCAB_PROMPT[:100]}...")

# ---------------- PART 2: SMART RESOLVER ----------------

class MarketTools:
    @staticmethod
    def search_ticker(query: str) -> Optional[str]:
        if DDGS is None: return None
        try:
            search_q = f"‡∏´‡∏∏‡πâ‡∏ô {query} stock symbol ticker settrade"
            with DDGS() as ddgs:
                results = list(ddgs.text(search_q, max_results=3))
                if not results: return None
                blob = " ".join([r.get("title", "") + " " + r.get("body", "") for r in results]).upper()
                candidates = re.findall(r"\b([A-Z]{2,8})\.BK\b", blob)
                if candidates: return candidates[0] + ".BK"
                candidates = re.findall(r"\b([A-Z]{2,8})\b", blob)
                blacklist = {"SET", "MAI", "BKK", "THAI", "PRICE", "NEWS", "STOCK", "TRADE", "DATA", "INFO"}
                for ticker in candidates:
                    if ticker not in blacklist and len(ticker) >= 2:
                        return ticker
        except: return None
        return None

    @staticmethod
    def verify_ticker(ticker: str) -> bool:
        if yf is None or not ticker: return False
        if "SET.BK" in ticker or "^" in ticker: return False
        try:
            return not yf.Ticker(ticker).history(period="5d").empty
        except: return False

class SmartMarketResolver:
    def __init__(self, context_mgr, cache_file=CACHE_FILE):
        self.memory = context_mgr.flat_memory.copy()
        self.cache_file = cache_file
        self.load_cache()

    def load_cache(self):
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    self.memory.update(json.load(f))
            except: pass

    def save_cache(self, mention, ticker):
        new_data = {}
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    new_data = json.load(f)
            except: pass
        new_data[mention.lower()] = ticker
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(new_data, f, ensure_ascii=False, indent=2)
        self.memory[mention.lower()] = ticker

    def resolve(self, mention):
        mention_key = mention.lower().strip()
        if mention_key in self.memory: return self.memory[mention_key]
        print(f"   üåê [Searching] '{mention}' ...")
        found = MarketTools.search_ticker(mention)
        time.sleep(1.5)
        if found:
            clean = found.upper().strip()
            if MarketTools.verify_ticker(clean):
                self.save_cache(mention_key, clean)
                return clean
            if ".BK" not in clean:
                thai = clean + ".BK"
                if MarketTools.verify_ticker(thai):
                    self.save_cache(mention_key, thai)
                    return thai
        return None

resolver = SmartMarketResolver(ctx_mgr)

# ---------------- PART 3: ASR & DOWNLOAD ----------------

def download_audio_from_youtube(url, output_dir):
    if not os.path.exists(output_dir): os.makedirs(output_dir)
    print("‚¨áÔ∏è  Downloading audio...")
    ydl_opts = {
        "format": "bestaudio/best",
        "postprocessors": [{"key": "FFmpegExtractAudio", "preferredcodec": "mp3", "preferredquality": "192"}],
        "outtmpl": os.path.join(output_dir, "%(id)s.%(ext)s"),
        "quiet": True, "no_warnings": True,
    }
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info = ydl.extract_info(url, download=True)
        filename = ydl.prepare_filename(info).replace(".webm", ".mp3").replace(".m4a", ".mp3")
        if not os.path.exists(filename): filename = os.path.join(output_dir, f"{info['id']}.mp3")
    return filename

from openai import OpenAI
try:
    http_client = httpx.Client(timeout=120.0)
    asr_client = OpenAI(base_url=TYPHOON_BASE_URL, api_key=TYPHOON_API_KEY, http_client=http_client)
except Exception as e:
    print(f"‚ùå Error clients: {e}"); raise SystemExit

def transcribe_chunk_safe(chunk_data, chunk_index, prompt):
    retries = 0
    while retries < 3:
        try:
            file_like = io.BytesIO(chunk_data)
            file_like.name = f"chunk_{chunk_index}.wav"
            response = asr_client.audio.transcriptions.create(
                model="typhoon-asr-realtime", file=file_like, language="th", prompt=prompt, temperature=0.2
            )
            print(f"   ‚úÖ [Chunk {chunk_index:02d}] Done.")
            return response.text
        except Exception as e:
            retries += 1; print(f"   ‚ö†Ô∏è [Chunk {chunk_index:02d}] Error: {e}"); time.sleep(1)
    return ""

def merge_transcriptions_fuzzy_overlap(transcripts):
    if not transcripts: return ""
    final_text = transcripts[0].strip()
    for i in range(1, len(transcripts)):
        prev = final_text
        curr = transcripts[i].strip()
        if not curr: continue
        matcher = difflib.SequenceMatcher(None, prev[-600:], curr[:600])
        match = matcher.find_longest_match(0, len(prev[-600:]), 0, 600)
        if match.size > 10: final_text += curr[match.b + match.size:]
        else: final_text += " " + curr
    return re.sub(r"\s+", " ", final_text).strip()

# ---------------- PART 4: LANGCHAIN AGENTS (CLEAN FORMAT) ----------------

llm = ChatOpenAI(base_url=GOOGLE_BASE_URL, api_key=GOOGLE_API_KEY, model=LLM_MODEL_NAME, temperature=0.15, max_tokens=4096)

class StockEntity(BaseModel):
    text_found: str = Field(...)
    predicted_ticker: Optional[str] = Field(None)
    confidence: str = Field(...)

class EntityList(BaseModel):
    entities: List[StockEntity]

# Chain 1: Cleaning
clean_prompt = ChatPromptTemplate.from_messages([
    ("system", "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ Transcript Editor ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà: ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ü‡∏∏‡πà‡∏°‡πÄ‡∏ü‡∏∑‡∏≠‡∏¢ (‡πÄ‡∏≠‡πà‡∏≠, ‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö) ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢ ‡πÅ‡∏ï‡πà‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤"),
    ("user", "Transcript:\n\"\"\"{raw_text}\"\"\"")
])
cleaning_chain = clean_prompt | llm | StrOutputParser()

# Chain 2: Correction (Logic + Ecosystem Check)
correction_system_prompt = (
    "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ 'Senior Investment Analyst' ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏´‡∏∏‡πâ‡∏ô'\n"
    "‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Transcript ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ **'‡∏ï‡∏£‡∏£‡∏Å‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏°‡∏ú‡∏•'**\n\n"
    
    "--- ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á (Sector Knowledge) ---\n"
    f"{SECTOR_CONTEXT_STR}\n\n"
    
    "--- ‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç (Correction Rules) ---\n"
    "1. **Neighbor Analysis**: ‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô‡∏°‡∏±‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô\n"
    "   - ‡πÄ‡∏à‡∏≠ JMART, SINGER -> ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ‡∏Ñ‡∏∑‡∏≠ **JMT** (‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏Å‡πâ‡πÄ‡∏õ‡πá‡∏ô MTC ‡∏´‡∏£‡∏∑‡∏≠ EMC)\n"
    "   - ‡πÄ‡∏à‡∏≠ SAWAD -> ‡∏Ñ‡∏π‡πà‡∏´‡∏π‡∏Ñ‡∏∑‡∏≠ **MTC** ‡∏´‡∏£‡∏∑‡∏≠ **TIDLOR**\n"
    "2. **Logic Check**: ‡∏ñ‡πâ‡∏≤‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÑ‡∏´‡∏ô ‡∏´‡∏∏‡πâ‡∏ô‡πÉ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏´‡∏∏‡πâ‡∏ô‡∏ú‡∏¥‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÇ‡∏ú‡∏•‡πà‡∏°‡∏≤\n"
    "3. **Phonetic**: ‡∏ñ‡πâ‡∏≤‡∏ú‡∏¥‡∏î Logic ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ä‡πà‡∏ß‡∏¢ (VANA->WHA, ‡∏Å‡∏£‡∏≤‡∏ü->GULF)\n\n"
    "**‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á**: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á 100% ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Transcript ‡∏â‡∏ö‡∏±‡∏ö‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå"
)
correction_prompt = ChatPromptTemplate.from_messages([
    ("system", correction_system_prompt),
    ("user", "Transcript to Correct:\n\"\"\"{clean_text}\"\"\"")
])
correction_chain = correction_prompt | llm | StrOutputParser()

# Chain 3: NER
ner_prompt = ChatPromptTemplate.from_messages([
    ("system", "‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô (Ticker) ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON"),
    ("user", "Text:\n\"\"\"{text}\"\"\"")
])
ner_chain = ner_prompt | llm.with_structured_output(EntityList)

# Chain 4: Summary (*** NO BOLD FORMAT ***)
summary_system_prompt = (
    "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ Content Editor ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡∏´‡∏∏‡πâ‡∏ô\n"
    "‡πÇ‡∏à‡∏ó‡∏¢‡πå: ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Markdown ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠ ‡πÇ‡∏î‡∏¢‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢\n\n"
    
    "--- ‡∏Å‡∏é‡πÄ‡∏´‡∏•‡πá‡∏Å‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö (Strict Formatting) ---\n"
    "1. **‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏≤ (Bold) ‡∏ó‡∏µ‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î**: (‡πÄ‡∏ä‡πà‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô **BEM**, ‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô BEM ‡πÄ‡∏â‡∏¢‡πÜ)\n"
    "2. **‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏µ‡∏¢‡∏á (Italic)**\n"
    "3. ‡πÉ‡∏ä‡πâ Header (#, ##) ‡πÅ‡∏ö‡πà‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠\n"
    "4. ‡πÉ‡∏ä‡πâ Bullet (-) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n\n"

    "--- ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á (Template) ---\n"
    "# ‡∏™‡∏£‡∏∏‡∏õ‡∏†‡∏≤‡∏ß‡∏∞‡∏ï‡∏•‡∏≤‡∏î\n\n"
    "## ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°\n"
    "- (‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏±‡πâ‡∏ô‡πÜ)\n\n"
    "## ‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à\n"
    "- (‡πÅ‡∏¢‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏•‡∏∏‡πà‡∏° ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏ó‡∏Ñ, ‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÑ‡∏ü‡πÅ‡∏ô‡∏ô‡∏ã‡πå)\n\n"
    "## ‡∏´‡∏∏‡πâ‡∏ô‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥\n"
    "- Ticker: (‡πÅ‡∏ô‡∏ß‡∏£‡∏±‡∏ö/‡πÅ‡∏ô‡∏ß‡∏ï‡πâ‡∏≤‡∏ô/‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå/‡∏ò‡∏µ‡∏°)\n"
    "  (‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß)\n\n"
    "## ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå\n"
    "- (‡∏™‡∏£‡∏∏‡∏õ‡∏ò‡∏µ‡∏°‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô)\n\n"
    "## ‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏°‡∏±‡∏î‡∏£‡∏∞‡∏ß‡∏±‡∏á (‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ 52 ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå)\n"
    "- (‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô)\n\n"
    "## ‡∏Ç‡πà‡∏≤‡∏ß‡∏≠‡∏∑‡πà‡∏ô‡πÜ\n"
    "- (‡∏Ç‡πà‡∏≤‡∏ß‡∏£‡∏≤‡∏¢‡∏ï‡∏±‡∏ß)\n\n"
    
    "[MAPPING]\n{mapping_str}"
)
summary_prompt = ChatPromptTemplate.from_messages([
    ("system", summary_system_prompt),
    ("user", "Transcript:\n\"\"\"{corrected_text}\"\"\"")
])
summary_chain = summary_prompt | llm | StrOutputParser()

# Chain 5: Audit (*** FINAL FORMAT CHECK ***)
audit_system_prompt = (
    "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ Editor-in-Chief ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏á‡∏≤‡∏ô‡∏ä‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢\n"
    "‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ Markdown\n"
    "1. **Format Check**: **‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏î‡∏≠‡∏Å‡∏à‡∏±‡∏ô (*) ‡∏•‡πâ‡∏≠‡∏°‡∏£‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô** (‡πÄ‡∏ä‡πà‡∏ô **BEM** ‡∏ú‡∏¥‡∏î, BEM ‡∏ñ‡∏π‡∏Å) -> ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÉ‡∏´‡πâ‡∏•‡∏ö‡∏≠‡∏≠‡∏Å‡∏ó‡∏±‡∏ô‡∏ó‡∏µ\n"
    "2. **Logic Check**: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏° Transcript\n"
    "3. **Output**: ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ‡∏â‡∏ö‡∏±‡∏ö‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå (Full Text)"
)
audit_prompt = ChatPromptTemplate.from_messages([
    ("system", audit_system_prompt),
    ("user", "Transcript:\n\"\"\"{transcript}\"\"\"\n\nDraft Summary:\n\"\"\"{markdown}\"\"\"")
])
audit_chain = audit_prompt | llm | StrOutputParser()

# ---------------- CONTROLLER ----------------

def run_ner_search_verify_agent(text: str):
    print("\nü§ñ [Step 3: NER] Extracting & Verifying...")
    try:
        res = ner_chain.invoke({"text": text[:35000]})
        entities = res.entities
    except: return []

    mappings = []
    seen = set()
    for ent in entities:
        if ent.text_found in seen: continue
        seen.add(ent.text_found)
        ticker = resolver.resolve(ent.text_found)
        if ticker:
            clean = ticker.replace('.BK', '')
            mappings.append(f"- {ent.text_found} -> {clean}")
            print(f"   ‚úÖ Mapped: {ent.text_found} -> {clean}")
    return "\n".join(mappings)

# ---------------- MAIN ----------------

def main():
    try:
        if not os.path.exists(TRANSCRIPT_OUTPUT_DIR): os.makedirs(TRANSCRIPT_OUTPUT_DIR)

        # 1. Download & ASR
        audio_path = download_audio_from_youtube(YOUTUBE_URL, DOWNLOAD_DIR)
        audio = AudioSegment.from_file(audio_path)
        print(f"\nüéß Audio: {len(audio)/1000.0:.1f}s")

        chunk_ms = CHUNK_DURATION_SEC * 1000
        chunks = []
        for i, s in enumerate(range(0, len(audio), chunk_ms - OVERLAP_DURATION_SEC*1000)):
            buf = io.BytesIO()
            audio[s:min(s+chunk_ms, len(audio))].export(buf, format="wav")
            chunks.append({"data": buf.getvalue(), "index": i})

        print(f"üöÄ Transcribing {len(chunks)} chunks...")
        results = {}
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
            futures = {ex.submit(transcribe_chunk_safe, c["data"], c["index"], ASR_VOCAB_PROMPT): c["index"] for c in chunks}
            for f in concurrent.futures.as_completed(futures): results[futures[f]] = f.result()
        
        raw_text = merge_transcriptions_fuzzy_overlap([results[i] for i in sorted(results.keys())])
        with open(os.path.join(TRANSCRIPT_OUTPUT_DIR, RAW_TRANSCRIPT_FILE), "w", encoding="utf-8") as f: f.write(raw_text)

        # 2. Cleaning
        print("\nüß† [Step 1] Cleaning Format...")
        time.sleep(2)
        clean_text = cleaning_chain.invoke({"raw_text": raw_text})

        # 3. Correction
        print("\nüß† [Step 2] Repairing ASR with Deep Logic...")
        time.sleep(5)
        corrected_text = correction_chain.invoke({"clean_text": clean_text})
        with open(os.path.join(TRANSCRIPT_OUTPUT_DIR, CLEAN_TRANSCRIPT_FILE), "w", encoding="utf-8") as f:
            f.write(corrected_text)

        # 4. NER & Verify
        mapping_str = run_ner_search_verify_agent(corrected_text)

        # 5. Summary
        print("\nüß† [Step 4] Summarizing...")
        time.sleep(5)
        draft = summary_chain.invoke({"corrected_text": corrected_text, "mapping_str": mapping_str})

        # 6. Audit
        print("\nüß† [Step 5] Auditing...")
        time.sleep(5)
        final_md = audit_chain.invoke({"transcript": corrected_text, "markdown": draft})

        with open(os.path.join(TRANSCRIPT_OUTPUT_DIR, MARKDOWN_FILE), "w", encoding="utf-8") as f:
            f.write(final_md)

        print(f"\n‚úÖ Done! Saved to {TRANSCRIPT_OUTPUT_DIR}")
        
        print("\n" + "="*20 + " FINAL TRANSCRIPT (CORRECTED) " + "="*20 + "\n")
        print(corrected_text)
        print("\n" + "="*22 + " END TRANSCRIPT " + "="*22 + "\n")

        print("\n" + "="*20 + " FINAL SUMMARY (CLEAN FORMAT) " + "="*20 + "\n")
        print(final_md)
        print("\n" + "="*25 + " END SUMMARY " + "="*25 + "\n")

    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback; traceback.print_exc()

if __name__ == "__main__":
    main()