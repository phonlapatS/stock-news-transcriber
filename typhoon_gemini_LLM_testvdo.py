import os
import io
import time
import random
import difflib
import re
import httpx
import json
from openai import OpenAI, RateLimitError, APITimeoutError
from pydub import AudioSegment
import concurrent.futures

# NEW: tools ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Agent
from duckduckgo_search import DDGS
import yfinance as yf

# ---------------- CONFIGURATION ----------------

# 1. Typhoon ASR
TYPHOON_BASE_URL = "https://api.opentyphoon.ai/v1"
TYPHOON_API_KEY = ""

# 2. Google Gemini (LLM ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NER + post-correction)
GOOGLE_BASE_URL = "https://generativelanguage.googleapis.com/v1beta/openai/"
GOOGLE_API_KEY = ""
LLM_MODEL_NAME = "gemini-2.0-flash"

# 3. Input / Output
LOCAL_AUDIO_FILE = "soundtest1/‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏∏‡πâ‡∏ô‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô_03112568.mp3"

# üìå vocab ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ASR ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡∏ä‡∏∏‡∏î‡πÄ‡∏•‡πá‡∏Å ‡πÜ)
ASR_VOCAB_FILE = "asr_vocab_data.json"   # ‡πÉ‡∏ä‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡πÉ‡∏´‡πâ ASR

# 4. Chunk Settings
CHUNK_DURATION_SEC = 45
OVERLAP_DURATION_SEC = 15
CHUNK_DURATION_MS = CHUNK_DURATION_SEC * 1000
OVERLAP_DURATION_MS = OVERLAP_DURATION_SEC * 1000

CACHE_DIR = "yt_cache"
TRANSCRIPT_OUTPUT_DIR = "transcripts_output"
TRANSCRIPT_PREFIX = "final_prod_agent"
MAX_WORKERS = 5

# ---------------- HELPER FUNCTIONS (DATA LOADING) ----------------

def load_domain_knowledge(filepath):
    """
    ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Knowledge Base ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå JSON ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ASR context ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    structure:
    {
        "investment_prompt": "...",   # optional
        "vocab_list": [
            {"term": "...", "desc": "...", "hints": [...], "logic": "..."},
            ...
        ]
    }
    """
    if not os.path.exists(filepath):
        print(f"‚ö†Ô∏è Warning: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {filepath} ‡πÉ‡∏ä‡πâ‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏ó‡∏ô")
        return {"investment_prompt": "", "vocab_list": []}

    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            vocab_len = len(data.get("vocab_list", []))
            print(f"‚úÖ Loaded ASR vocab from {filepath}: {vocab_len} items.")
            if data.get("investment_prompt"):
                print(f"‚úÖ Found investment_prompt in {filepath}")
            return data
    except json.JSONDecodeError as e:
        print(f"‚ùå Error: ‡πÑ‡∏ü‡∏•‡πå {filepath} ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ({e})")
        return {"investment_prompt": "", "vocab_list": []}
    except Exception as e:
        print(f"‚ùå Error loading vocab file {filepath}: {e}")
        return {"investment_prompt": "", "vocab_list": []}

# 4. LOAD KNOWLEDGE BASE (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ASR)
ASR_KNOWLEDGE = load_domain_knowledge(ASR_VOCAB_FILE)

# ---------------- INIT CLIENTS ----------------

try:
    http_client = httpx.Client(timeout=120.0)
    asr_client = OpenAI(
        base_url=TYPHOON_BASE_URL,
        api_key=TYPHOON_API_KEY,
        http_client=http_client
    )
    llm_client = OpenAI(
        base_url=GOOGLE_BASE_URL,
        api_key=GOOGLE_API_KEY,
        http_client=http_client
    )
except Exception as e:
    print(f"‚ùå Error initializing Clients: {e}")
    raise SystemExit

# ---------------- ASR CONTEXT BIASING ----------------

def build_asr_prompt_from_kb(kb, extra_context=None):
    """
    ‡πÉ‡∏ä‡πâ ASR vocab (‡∏à‡∏≤‡∏Å asr_vocab_data.json) ‡∏°‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ASR:
    - ‡∏î‡∏∂‡∏á investment_prompt ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
    - ‡∏î‡∏∂‡∏á term ‡πÅ‡∏•‡∏∞ hints ‡∏°‡∏≤‡∏£‡∏ß‡∏°‡πÄ‡∏õ‡πá‡∏ô keyword
    ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ vocab ‡∏ä‡∏∏‡∏î‡πÄ‡∏•‡πá‡∏Å (‡∏´‡∏∏‡πâ‡∏ô‡∏¢‡∏≠‡∏î‡∏Æ‡∏¥‡∏ï + ‡∏®‡∏±‡∏û‡∏ó‡πå‡∏´‡∏•‡∏±‡∏Å) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ prompt ‡∏ö‡∏ß‡∏°‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
    """
    base = kb.get("investment_prompt", "") or ""
    terms = []
    for item in kb.get("vocab_list", []):
        term = item.get("term")
        if term:
            terms.append(term)
        for h in item.get("hints", []) or []:
            terms.append(h)

    cleaned = []
    seen = set()
    for p in terms:
        p = (p or "").strip()
        if not p:
            continue
        if p not in seen:
            seen.add(p)
            cleaned.append(p)

    keywords_str = " ".join(cleaned)
    ctx = extra_context.strip() + " " if extra_context else ""
    if base.strip():
        prompt = f"{ctx}{base.strip()} {keywords_str}".strip()
    else:
        fallback_ctx = (
            "‡∏ö‡∏£‡∏¥‡∏ö‡∏ó: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏∏‡πâ‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡πÄ‡∏ô‡πâ‡∏ô‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á‡∏´‡∏∏‡πâ‡∏ô‡πÑ‡∏ó‡∏¢ SET Index ‡∏î‡∏≠‡∏Å‡πÄ‡∏ö‡∏µ‡πâ‡∏¢ Fed "
        )
        prompt = f"{fallback_ctx} {keywords_str}".strip()

    print(f"üß© ASR prompt length: {len(prompt)} characters")
    return prompt

# ‡πÉ‡∏ä‡πâ ASR_KNOWLEDGE ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Typhoon
INVESTMENT_PROMPT = build_asr_prompt_from_kb(
    ASR_KNOWLEDGE,
    extra_context="‡∏ö‡∏£‡∏¥‡∏ö‡∏ó: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏∏‡πâ‡∏ô‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô ‡πÄ‡∏ô‡πâ‡∏ô‡∏°‡∏∏‡∏°‡∏°‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡∏™‡∏±‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏•‡∏≤‡∏á"
)

# ---------------- CORE FUNCTIONS ----------------

def get_unique_output_path(prefix, directory, extension=".txt"):
    if not os.path.exists(directory):
        os.makedirs(directory)
    i = 1
    while True:
        filename = f"{prefix}_{i:02d}{extension}"
        full_path = os.path.join(directory, filename)
        if not os.path.exists(full_path):
            return full_path
        i += 1

def transcribe_chunk_safe(chunk_data, chunk_index, prompt, max_retries=5):
    print(f"   ‚ñ∂Ô∏è  [Chunk {chunk_index:02d}] Transcribing...")
    retries = 0
    while retries < max_retries:
        try:
            file_like = io.BytesIO(chunk_data)
            file_like.name = f"chunk_{chunk_index}.wav"
            response = asr_client.audio.transcriptions.create(
                model="typhoon-asr-realtime",
                file=file_like,
                language="th",
                prompt=prompt,
            )
            print(f"   ‚úÖ [Chunk {chunk_index:02d}] Done.")
            return response.text
        except (RateLimitError, APITimeoutError):
            retries += 1
            wait_time = (2 * (2 ** retries)) + random.random()
            print(f"   ‚è≥ [Chunk {chunk_index:02d}] Retry {retries}/{max_retries} in {wait_time:.1f}s...")
            time.sleep(wait_time)
        except Exception as e:
            print(f"   ‚ùå [Chunk {chunk_index:02d}] Error: {e}")
            return ""
    return ""

def merge_transcriptions_fuzzy_overlap(transcripts):
    if not transcripts:
        return ""
    final_text = transcripts[0].strip()
    for i in range(1, len(transcripts)):
        prev_chunk = final_text
        curr_chunk = transcripts[i].strip()
        if not curr_chunk:
            continue

        check_len = 400
        prev_suffix = prev_chunk[-check_len:] if len(prev_chunk) > check_len else prev_chunk
        search_range = min(len(curr_chunk), check_len)

        matcher = difflib.SequenceMatcher(None, prev_suffix, curr_chunk[:search_range])
        match = matcher.find_longest_match(0, len(prev_suffix), 0, search_range)

        if match.size > 15:
            trim_idx = match.b + match.size
            text_to_append = curr_chunk[trim_idx:]
            final_text += text_to_append
        else:
            final_text += " " + curr_chunk

    return final_text.replace("  ", " ").strip()

def clean_fillers_and_repetition(text):
    if not text:
        return ""
    fillers = [r"‡πÄ‡∏≠‡πà‡∏≠+", r"‡∏≠‡πà‡∏≤+", r"‡∏≠‡∏∑‡∏°+", r"‡∏≠‡πã‡∏≠+", r"‡∏≠‡∏≠+", r"‡πÅ‡∏ö‡∏ö‡∏ß‡πà‡∏≤", r"‡∏Ñ‡∏∑‡∏≠‡πÅ‡∏ö‡∏ö"]
    for filler in fillers:
        text = re.sub(filler, "", text)

    phrases = re.split(r'[\n]+', text)
    cleaned_phrases = []

    for phrase in phrases:
        phrase = phrase.strip()
        if not phrase:
            continue

        is_duplicate = False
        lookback_count = 5
        start_check = max(0, len(cleaned_phrases) - lookback_count)
        for prev_phrase in cleaned_phrases[start_check:]:
            if difflib.SequenceMatcher(None, prev_phrase, phrase).ratio() > 0.85:
                is_duplicate = True
                break

        if not is_duplicate:
            cleaned_phrases.append(phrase)

    return "\n".join(cleaned_phrases)

# ---------------- AGENT: NER + TOOLS ----------------

def extract_financial_entities_with_llm(raw_text):
    """
    ‡πÉ‡∏ä‡πâ Gemini ‡∏î‡∏∂‡∏á entity ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡∏à‡∏≤‡∏Å transcript
    ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON list ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô:
    [
      {"mention": "‡πÑ‡∏ó‡∏¢‡∏Ñ‡∏°", "type": "stock_th", "note": ""},
      {"mention": "‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢", "type": "stock_th", "note": "bank"},
      {"mention": "SET Index", "type": "index", "note": ""}
    ]
    """
    system_prompt = (
        "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ NER agent ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏∏‡πâ‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\n"
        "‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏î‡∏∂‡∏á '‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô' ‡∏à‡∏≤‡∏Å transcript ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà:\n"
        "- ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô‡πÑ‡∏ó‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô\n"
        "- ‡∏ä‡∏∑‡πà‡∏≠‡∏î‡∏±‡∏ä‡∏ô‡∏µ‡∏ï‡∏•‡∏≤‡∏î‡∏´‡∏∏‡πâ‡∏ô (‡πÄ‡∏ä‡πà‡∏ô SET Index)\n"
        "- ‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≠‡∏á‡∏ó‡∏∏‡∏ô‡∏´‡∏£‡∏∑‡∏≠ ETF ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ\n"
        "‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON array ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏≠‡∏∑‡πà‡∏ô\n"
        "‡πÅ‡∏ï‡πà‡∏•‡∏∞ object ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ key: mention, type, note\n"
        "type ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡πá‡∏ô 'stock_th', 'company', 'index', 'fund', 'other_financial'\n"
        "‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö entity ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô []"
    )

    user_prompt = f"""
Transcript (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢):

\"\"\"{raw_text}\"\"\"

‡πÉ‡∏´‡πâ‡∏≠‡∏≠‡∏Å‡∏ú‡∏•‡πÄ‡∏õ‡πá‡∏ô JSON ‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
"""

    try:
        resp = llm_client.chat.completions.create(
            model=LLM_MODEL_NAME,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.0,
            max_tokens=1024
        )
        content = resp.choices[0].message.content.strip()
        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏î‡∏∂‡∏á JSON ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å text
        # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏ì‡∏µ model ‡πÉ‡∏™‡πà ```json ... ```
        json_str = content
        if "```" in content:
            # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏ô code block
            m = re.search(r"```(?:json)?(.*?)```", content, re.S)
            if m:
                json_str = m.group(1).strip()
        entities = json.loads(json_str)
        if not isinstance(entities, list):
            print("‚ö†Ô∏è NER output is not a list, fallback []")
            return []
        print(f"üß© NER Agent found {len(entities)} entities")
        return entities
    except Exception as e:
        print(f"‚ùå NER Agent Error: {e}")
        return []

def guess_ticker_from_ddg(name_th, max_results=5):
    """
    ‡πÉ‡∏ä‡πâ DuckDuckGo ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏î‡∏≤ ticker ‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô (‡πÄ‡∏ä‡πà‡∏ô KBANK.BK)
    ‡πÄ‡∏õ‡πá‡∏ô heuristic ‡∏á‡πà‡∏≤‡∏¢ ‡πÜ ‡∏û‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô context ‡πÉ‡∏´‡πâ LLM ‡πÑ‡∏î‡πâ
    """
    query = f"{name_th} ‡∏´‡∏∏‡πâ‡∏ô ‡πÑ‡∏ó‡∏¢"
    with DDGS() as ddgs:
        results = list(ddgs.text(query, max_results=max_results))

    ticker_candidates = []
    pattern = re.compile(r"\b([A-Z]{2,6}\.BK)\b")
    for r in results:
        text = " ".join([
            str(r.get("title", "")),
            str(r.get("body", "")),
            str(r.get("href", "")),
        ])
        for m in pattern.findall(text):
            ticker_candidates.append(m)

    ticker_candidates = list(dict.fromkeys(ticker_candidates))  # dedupe ‡πÅ‡∏ï‡πà‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏•‡∏≥‡∏î‡∏±‡∏ö
    return ticker_candidates[0] if ticker_candidates else None

def validate_ticker_with_yfinance(ticker):
    """
    ‡πÄ‡∏ä‡πá‡∏Å‡∏ß‡πà‡∏≤‡∏ï‡∏±‡∏ß ticker ‡∏û‡∏≠‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏° ‡∏î‡πâ‡∏ß‡∏¢ yfinance ‡πÅ‡∏ö‡∏ö‡πÄ‡∏ö‡∏≤ ‡πÜ
    """
    if not ticker:
        return False
    try:
        t = yf.Ticker(ticker)
        _ = t.fast_info  # ‡∏ñ‡πâ‡∏≤ call ‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏°‡πà‡∏û‡∏±‡∏á ‡πÅ‡∏õ‡∏•‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏´‡∏ô‡∏∂‡πà‡∏á
        return True
    except Exception:
        return False

def enrich_entities_with_tools(entities):
    """
    ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö entity ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏∏‡πâ‡∏ô/‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ DuckDuckGo + yfinance ‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°
    return ‡πÄ‡∏õ‡πá‡∏ô list ‡∏Ç‡∏≠‡∏á mapping ‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô context ‡πÉ‡∏´‡πâ LLM
    ‡πÄ‡∏ä‡πà‡∏ô:
    [
      {"mention": "‡πÑ‡∏ó‡∏¢‡∏Ñ‡∏°", "ticker": "THCOM.BK", "name_en": "THAICOM PCL", "source": "ddg+yf"},
      ...
    ]
    """
    enriched = []
    for e in entities:
        e_type = e.get("type")
        mention = e.get("mention")
        if not mention:
            continue

        if e_type not in ["stock_th", "company"]:
            # ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ enrich ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏´‡∏∏‡πâ‡∏ô/‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó
            enriched.append({
                "mention": mention,
                "type": e_type,
                "ticker": None,
                "name_en": None,
                "source": "ner_only",
            })
            continue

        print(f"üîç Enriching entity: {mention} ({e_type})")
        ticker = guess_ticker_from_ddg(mention)
        if ticker and validate_ticker_with_yfinance(ticker):
            name_en = None
            try:
                t = yf.Ticker(ticker)
                info = getattr(t, "fast_info", None)
                # ‡∏ö‡∏≤‡∏á‡∏ó‡∏µ‡∏à‡∏∞‡∏°‡∏µ shortName ‡∏´‡∏£‡∏∑‡∏≠ longName ‡πÉ‡∏ô .info ‡πÅ‡∏ï‡πà‡∏≠‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏à‡∏ä‡πâ‡∏≤
                # ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ä‡∏∑‡πà‡∏≠ EN ‡∏à‡∏£‡∏¥‡∏á ‡πÜ ‡∏Ñ‡πà‡∏≠‡∏¢‡∏Ç‡∏¢‡∏≤‡∏¢‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á
            except Exception:
                info = None
            enriched.append({
                "mention": mention,
                "type": e_type,
                "ticker": ticker,
                "name_en": name_en,
                "source": "ddg+yf",
            })
        else:
            enriched.append({
                "mention": mention,
                "type": e_type,
                "ticker": None,
                "name_en": None,
                "source": "ddg_only_or_failed",
            })

    print(f"‚úÖ Enriched {len(enriched)} entities")
    return enriched

def build_entity_context_for_llm(enriched_entities):
    """
    ‡πÅ‡∏õ‡∏•‡∏á mapping ‡∏ó‡∏µ‡πà enrich ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô context ‡πÄ‡∏ö‡∏≤ ‡πÜ ‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ Gemini
    """
    if not enriched_entities:
        return "‡πÑ‡∏°‡πà‡∏°‡∏µ mapping ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô‡∏à‡∏≤‡∏Å agent/tools ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö transcript ‡∏ô‡∏µ‡πâ"

    lines = ["ENTITY MAPPING ‡∏à‡∏≤‡∏Å Agent + Tools (‡πÉ‡∏ä‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å):"]
    for e in enriched_entities:
        mention = e.get("mention")
        etype = e.get("type")
        ticker = e.get("ticker") or "UNKNOWN"
        src = e.get("source")
        line = f"- mention: {mention} | type: {etype} | ticker: {ticker} | source: {src}"
        lines.append(line)
    return "\n".join(lines)

# ---------------- LLM CORRECTION (‡πÉ‡∏ä‡πâ Agent context) ----------------

def correct_transcript_with_llm(raw_text, enriched_entities):
    print(f"\nüß† Sending to Gemini ({LLM_MODEL_NAME}) for Logical Reconstruction with Agent Context...")

    # 1) ‡∏•‡∏ö trigger ‡∏ó‡∏µ‡πà‡∏ä‡∏ß‡∏ô‡πÉ‡∏´‡πâ model ‡πÄ‡∏û‡πâ‡∏≠
    hallucination_triggers = ["Subtitles by", "Amara.org", "Unidentified speaker"]
    for trigger in hallucination_triggers:
        raw_text = raw_text.replace(trigger, "")

    # 2) ‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå filler + ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ã‡πâ‡∏≥‡∏Å‡πà‡∏≠‡∏ô
    raw_text = clean_fillers_and_repetition(raw_text)

    # 3) ‡πÅ‡∏õ‡∏•‡∏á entity mapping ‡πÄ‡∏õ‡πá‡∏ô context ‡πÄ‡∏ö‡∏≤ ‡πÜ
    entity_ctx = build_entity_context_for_llm(enriched_entities)

    system_prompt = (
        "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ AI Financial Reconstruction Engine ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏∏‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô\n"
        "‡∏ó‡∏∏‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏∑‡∏≠‡∏ö‡∏ó‡∏û‡∏π‡∏î‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô ‡∏´‡∏∏‡πâ‡∏ô ‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à ‡πÅ‡∏•‡∏∞‡∏ï‡∏•‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô\n"
        "‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á transcript ‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å ASR ‡πÅ‡∏•‡∏∞ ENTITY MAPPING ‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏à‡∏≤‡∏Å Agent + Tools\n"
        "‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ ENTITY MAPPING ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô/‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ticker ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°\n"
        "‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ ENTITY MAPPING ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÉ‡∏´‡∏°‡πà ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å\n"
        "‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì:\n"
        "- ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡πà‡∏≤‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á ‡∏°‡∏µ‡∏ï‡∏£‡∏£‡∏Å‡∏∞‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á ‡πÉ‡∏ä‡πâ‡∏®‡∏±‡∏û‡∏ó‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\n"
        "- ‡πÇ‡∏ü‡∏Å‡∏±‡∏™‡∏ó‡∏µ‡πà‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô ‡∏ï‡∏±‡∏î small talk ‡∏≠‡∏≠‡∏Å ‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏∏‡∏°‡∏°‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö\n"
    )

    user_prompt = f"""
‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠ ENTITY MAPPING ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Agent + DuckDuckGo + yfinance:

{entity_ctx}


‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å ASR (‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Ñ‡∏≥‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô / ‡∏õ‡∏µ‡∏ú‡∏¥‡∏î / ‡∏û‡∏π‡∏î‡∏ã‡πâ‡∏≥ / ‡∏Ñ‡∏≥‡∏ü‡∏∏‡πà‡∏°‡πÄ‡∏ü‡∏∑‡∏≠‡∏¢):

\"\"\"{raw_text}\"\"\"


‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì:
1. ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡∏≠‡πà‡∏≤‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á ‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏ï‡∏£‡∏£‡∏Å‡∏∞‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '‡∏ö‡∏ó‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏∏‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô'
2. ‡πÅ‡∏Å‡πâ‡∏Ñ‡∏≥‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô/‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î‡πÉ‡∏´‡πâ‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ENTITY MAPPING ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô/ ticker
3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏°‡∏ú‡∏•‡∏Ç‡∏≠‡∏á‡∏õ‡∏µ / ‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™ / ‡∏ï‡∏±‡∏ß‡∏¢‡πà‡∏≠‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô ‡∏ï‡∏≤‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à‡πÉ‡∏´‡πâ‡∏Ñ‡∏á‡πÑ‡∏ß‡πâ‡∏ï‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°
4. ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô‡πÑ‡∏ó‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Ticker ‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏ç‡πà‡∏ï‡∏≤‡∏° mapping ‡∏ñ‡πâ‡∏≤ ENTITY MAPPING ‡∏°‡∏µ ticker ‡πÉ‡∏´‡πâ‡πÅ‡∏•‡πâ‡∏ß
5. ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á '‡πÄ‡∏≠‡πà‡∏≠', '‡∏≠‡πà‡∏≤', '‡∏Ñ‡∏∑‡∏≠‡πÅ‡∏ö‡∏ö' ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô
6. ‡∏≠‡∏¢‡πà‡∏≤‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö ‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏õ‡πâ‡∏≤‡∏£‡∏≤‡∏Ñ‡∏≤‡πÉ‡∏´‡∏°‡πà ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÉ‡∏´‡∏°‡πà ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ã‡∏∑‡πâ‡∏≠/‡∏Ç‡∏≤‡∏¢‡πÉ‡∏´‡∏°‡πà
7. ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏â‡∏ö‡∏±‡∏ö‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°
"""

    try:
        response = llm_client.chat.completions.create(
            model=LLM_MODEL_NAME,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.15,
            max_tokens=4096
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"‚ùå LLM Error: {e}")
        # fallback ‡πÄ‡∏õ‡πá‡∏ô raw_text ‡∏ó‡∏µ‡πà‡∏•‡πâ‡∏≤‡∏á filler ‡πÅ‡∏•‡πâ‡∏ß
        return raw_text

# ---------------- MAIN EXECUTION ----------------

def main():
    try:
        if not os.path.exists(LOCAL_AUDIO_FILE):
            print(f"‚ùå File not found: {LOCAL_AUDIO_FILE}")
            return

        print(f"\nüéß Processing: {LOCAL_AUDIO_FILE}")
        audio = AudioSegment.from_file(LOCAL_AUDIO_FILE)

        duration_sec = len(audio) / 1000
        print(f"** Duration: {int(duration_sec//3600):02d}:{int((duration_sec%3600)//60):02d}:{duration_sec%60:05.2f}")

        print(f"üì¶ Chunking (Chunk={CHUNK_DURATION_SEC}s, Overlap={OVERLAP_DURATION_SEC}s)...")
        chunks = []
        start = 0
        idx = 0
        while start < len(audio):
            end = min(start + CHUNK_DURATION_MS, len(audio))
            chunk = audio[start:end]
            buf = io.BytesIO()
            chunk.export(buf, format="wav")
            chunks.append({'data': buf.getvalue(), 'index': idx})
            if end == len(audio):
                break
            start += (CHUNK_DURATION_MS - OVERLAP_DURATION_MS)
            idx += 1
        print(f"‚úÖ Created {len(chunks)} chunks.")

        print(f"üöÄ Starting Transcription with ASR prompt biasing...")
        results = {}
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_idx = {
                executor.submit(
                    transcribe_chunk_safe,
                    c['data'],
                    c['index'],
                    INVESTMENT_PROMPT
                ): c['index']
                for c in chunks
            }
            for future in concurrent.futures.as_completed(future_to_idx):
                idx = future_to_idx[future]
                try:
                    results[idx] = future.result()
                except Exception:
                    results[idx] = ""

        print("üîÑ Merging text...")
        sorted_text = [results[i] for i in sorted(results.keys())]
        raw_transcript = merge_transcriptions_fuzzy_overlap(sorted_text)

        print("\nüßπ Raw transcript after merge (before Agent/LLM):")
        print("-" * 40)
        print(raw_transcript[:1000], "..." if len(raw_transcript) > 1000 else "")
        print("-" * 40)

        # STEP ‡πÉ‡∏´‡∏°‡πà: Agent NER + Tools
        entities = extract_financial_entities_with_llm(raw_transcript)
        enriched_entities = enrich_entities_with_tools(entities)

        # STEP ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: LLM correction ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ entity mapping ‡πÅ‡∏ó‡∏ô vocab JSON ‡∏¢‡∏±‡∏Å‡∏©‡πå
        final_output = correct_transcript_with_llm(raw_transcript, enriched_entities)

        print("\n" + "=" * 40)
        print("üìÑ --- FINAL TRANSCRIPTION RESULT ---")
        print("=" * 40)
        print(final_output)
        print("=" * 40 + "\n")

        out_path = get_unique_output_path(TRANSCRIPT_PREFIX, TRANSCRIPT_OUTPUT_DIR)
        with open(out_path, "w", encoding="utf-8") as f:
            f.write(final_output)
        print(f"‚úÖ Saved result to: {out_path}")

    except Exception as e:
        print(f"‚ùå Main Error: {e}")

if __name__ == "__main__":
    main()
