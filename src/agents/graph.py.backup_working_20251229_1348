# src/agents/graph.py
from typing import TypedDict, List, Dict, Any
import os  # For file operations in auto_learning
import json  # For JSON operations in auto_learning  
from datetime import datetime  # For timestamps in auto_learning

# Import optional libs
try:
    from thefuzz import process as fuzzy_process # type: ignore
except ImportError:
    fuzzy_process = None

from langgraph.graph import StateGraph, END
from src.core.data_managers import StockContextManager, FinanceTermManager, SmartMarketResolver
from src.agents.llm_prompts import correction_chain, combined_verification_chain, summary_chain  # Re-added
from src.agents.workflow_logic import process_large_text_robust
from config import MAX_RETRIES

# Import 3-node validation pipeline (NEW)
from src.agents.validation_pipeline import (
    entity_tagger_node,
    market_validator_node,
    content_repair_node
)

# --- State Definition ---
class AgentState(TypedDict):
    raw_transcript: str
    channel_name: str
    chunk_size: int
    current_text: str       # Text ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏Å‡πâ
    feedback_msg: str       # Feedback ‡∏à‡∏≤‡∏Å‡∏£‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô
    iteration_count: int    # ‡∏ô‡∏±‡∏ö‡∏£‡∏≠‡∏ö
    verified_mappings: List[str]
    unknown_tickers: List[str]
    correction_suggestions: List[str] # ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏à‡∏≤‡∏Å Fact-Checker
    chunks_with_issues: List[int] # [NEW] ‡πÄ‡∏Å‡πá‡∏ö index ‡∏Ç‡∏≠‡∏á chunk ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
    verified_entities_cache: Dict[str, str] # Cache ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö Entity ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡πâ‡∏ß (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: {"macisen seven": "Magnificent Seven"})
    # --- 3-Node Pipeline Fields ---
    extracted_entities: List[Dict] # Output from Entity Tagger: [{'entity': 'PTT', 'price': 35.0, 'type': 'support'}]
    validated_data: Dict           # Validated data from yfinance: {'PTT': {'market_price': 34.5, 'status': 'mismatch'}}
    validation_logs: List[str]     # Logs for auto-learning/asr_errors.json
    # --- Original Fields ---
    quality_score: int      # ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏†‡∏≤‡∏©‡∏≤ (1-10)
    final_summary: str      # ‡∏ú‡∏•‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Markdown
    data_managers: Dict[str, Any] # Dictionary ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö instances ‡∏Ç‡∏≠‡∏á Manager ‡∏ï‡πà‡∏≤‡∏á‡πÜ

# === AGENT GRAPH NODES ===

def correction_node(state: AgentState):
    """
    Node: ‡∏ô‡∏±‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç (Editor)

    ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (current_text) ‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞ (feedback_msg) ‡∏à‡∏≤‡∏Å‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
    ‡πÅ‡∏•‡πâ‡∏ß‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ LLM (correction_chain) ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏´‡πâ‡∏™‡∏•‡∏∞‡∏™‡∏•‡∏ß‡∏¢‡πÅ‡∏•‡∏∞‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Context ‡∏à‡∏≤‡∏Å StockContextManager ‡πÅ‡∏•‡∏∞ FinanceTermManager ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
    ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏®‡∏±‡∏û‡∏ó‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏≤‡∏á

    Args:
        state (AgentState): ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏Ç‡∏≠‡∏á Graph

    Returns:
        dict: ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏î‡πâ‡∏ß‡∏¢ 'current_text' ‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° 'iteration_count'
    """
    print(f"\nüîÑ [Node] Correction (Round {state['iteration_count'] + 1})...")
    # ‡∏î‡∏∂‡∏á Manager ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏à‡∏≤‡∏Å State ‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
    ctx_mgr = state["data_managers"]["stock_context"]
    term_mgr = state["data_managers"]["finance_term"]
    rate_limiter = state.get("rate_limiter")  # Get rate limiter if available
    
    # [NEW] Apply pre-processing before LLM (if not already done)
    current_text = state["current_text"]
    if state['iteration_count'] == 0:  # Only on first iteration
        try:
            from src.utils.preprocessor import TextPreprocessor
            preprocessor = TextPreprocessor()
            current_text, _ = preprocessor.preprocess(current_text, verbose=False)
            
            # Apply context-aware resolution
            from src.utils.context_aware_resolver import ContextAwareResolver
            context_resolver = ContextAwareResolver(ctx_mgr, term_mgr)
            current_text, _ = context_resolver.fix_context_blindness(current_text)
        except Exception as e:
            print(f"   ‚ö†Ô∏è Pre-processing in correction node failed: {e}")
    
    # Load learned errors for continuous learning
    from src.agents.llm_prompts import get_learned_errors_section
    learned_errors = get_learned_errors_section()

    # Combine feedback and suggestions
    feedback = state["feedback_msg"]
    if state.get("correction_suggestions"):
        feedback += "\n" + "\n".join(state["correction_suggestions"])
    
    corrected = process_large_text_robust(
        text=current_text,  # Use pre-processed text
        chain=correction_chain,
        chunk_size=state["chunk_size"],
        rate_limiter=rate_limiter,
        extra_inputs={
            "sector_context": ctx_mgr.get_sector_prompt_str(),
            "domain_terms_context": term_mgr.get_prompt_str(),
            "learned_errors": learned_errors,
            "feedback_msg": feedback,
            "channel_name": state["channel_name"]
        }
    )
    
    # [IMPROVED] Enhanced Deduplication - DISABLED FOR TESTING
    print("\nüßπ Deduplication...")
    print("   ‚ö†Ô∏è  DEDUP DISABLED - Testing without deduplication")
    
    # DISABLED: Comment out all dedup to test if it's causing content loss
    #try:
    if False:  # Disabled for testing
        from src.utils.text_deduplication import remove_duplicate_sentences_smart, remove_chunk_boundary_duplicates
        from src.utils.dedup_config_manager import DedupConfigManager
        from src.utils.dedup_marker_processor import remove_duplicates as remove_manual_duplicates
        
        # Step 0: Remove MANUAL [DUP] markers from LLM first (User requested)
        corrected, manual_removed = remove_manual_duplicates(corrected)
        if manual_removed > 0:
            print(f"   üßπ Manual [DUP] markers: {manual_removed} sentences removed")

        # Get adaptive thresholds based on content type
        thresholds = DedupConfigManager.get_thresholds(
            channel_name=state.get("channel_name", ""),
            video_title=""  # Could add video_title to state in future
        )
        
        # Step 1: Boundary deduplication FIRST (stricter, exact matches from chunk overlap)
        corrected, boundary_removed = remove_chunk_boundary_duplicates(
            corrected, 
            similarity_threshold=thresholds["boundary"]
        )
        if boundary_removed > 0:
            print(f"   üßπ Boundary dedup: {boundary_removed} duplicate(s) removed (threshold={thresholds['boundary']})")
        
        # Step 2: General deduplication SECOND (looser, for other duplicates)
        corrected, num_removed = remove_duplicate_sentences_smart(
            corrected, 
            similarity_threshold=thresholds["general"]
        )
        if num_removed > 0:
            print(f"   üßπ General dedup: {num_removed} duplicate(s) removed (threshold={thresholds['general']})")
            
    #except Exception as e:
    #    print(f"   ‚ö†Ô∏è Deduplication failed (continuing): {e}")
    
    # ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡πÄ‡∏Å‡πà‡∏≤‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡∏ã‡πâ‡∏≥‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
    # SAFE DEDUPLICATION: Remove exact duplicates only
    try:
        from src.utils.safe_dedup import safe_deduplicate
        
        print("\nüßπ [Safe Dedup] Removing exact duplicates...")
        deduplicated_text, dedup_stats = safe_deduplicate(
            corrected,  # FIX: Use 'corrected' variable
            max_removal_percent=0.10  # Max 10% removal
        )
        
        if dedup_stats["applied"]:
            corrected = deduplicated_text  # FIX: Assign back to 'corrected'
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è Dedup error (skipping): {e}")
    
    return {
        "current_text": corrected,  # FIX: Return 'corrected'
        "iteration_count": state["iteration_count"] + 1,
        "correction_suggestions": []
    }


def fact_checking_node(state: AgentState):
    """
    Node: Fact Checker - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö CLEAN output ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å correction ‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡πà‡∏≤:
    1. ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ï‡πâ‡∏≠‡∏á‡∏´‡πâ‡∏≤‡∏° (Fed, etc.)
    2. ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô‡∏Å‡∏≥‡∏Å‡∏±‡∏ö (Price-Ticker Consistency)
    3. ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏ï‡∏•‡∏≤‡∏î‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (Market Validation)
   4. ‡πÑ‡∏°‡πà‡∏°‡∏µ Hallucination
    
    ‡∏ñ‡πâ‡∏≤‡∏û‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ‡∏à‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° feedback ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ correction_node ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
    """
    print(f"\nüîç [Node] Fact Checking...")
    
    try:
        from src.validation.fact_checker import CleanFactChecker
        
        checker = CleanFactChecker()
        
        # Extract filename from state (for date extraction)
        filename = state.get('video_metadata', {}).get('title', '')
        if not filename:
            # Fallback: try to construct from other metadata
            filename = state.get('filename', '')
        
        result = checker.validate(state["current_text"], filename=filename)
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
        if not result['is_valid']:
            print("   ‚ùå Validation Failed!")
            for error in result['errors'][:3]:  # ‡πÅ‡∏™‡∏î‡∏á 3 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏£‡∏Å
                print(f"      {error}")
        else:
            print("   ‚úÖ Validation Passed!")
        
        # ‡πÅ‡∏™‡∏î‡∏á warnings
        if result['warnings']:
            print(f"   ‚ö†Ô∏è  {len(result['warnings'])} warnings found")
            for warning in result['warnings'][:2]:  # ‡πÅ‡∏™‡∏î‡∏á 2 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏£‡∏Å
                print(f"      {warning}")
        
        # ‡πÅ‡∏™‡∏î‡∏á market validation stats
        stats = result['statistics']
        if stats.get('recording_date'):
            print(f"   üìÖ Recording Date: {stats['recording_date']}")
            print(f"   üìä Market Validated: {stats.get('market_validated', 0)}/{stats.get('ticker_price_pairs', 0)} ticker-price pairs")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
        return {
            "quality_score": 10 if result['is_valid'] else max(1, 10 - len(result['errors'])),
            "fact_check_passed": result['is_valid'],
            "fact_check_warnings": len(result['warnings']),
            "fact_check_errors": len(result['errors']),
            "market_validation_count": len(result.get('market_validation_issues', []))
        }
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è Fact checking failed: {e}")
        import traceback
        traceback.print_exc()
        return {"quality_score": state.get("quality_score", 7)}


def auto_learning_node(state: AgentState):
    """
    Node: Auto-Learning from Corrections
    
    Compares RAW vs CORRECTED text to extract what was fixed,
    then automatically updates knowledge bases for future use.
    
    NOW ALSO: Logs validation issues from market_validator to asr_errors.json
    
    This creates a sustainable, self-improving system.
    """
    print(f"\nüìö [Node] Auto-Learning...")
    
    try:
        from src.utils.auto_learning_manager import AutoLearningManager
        
        raw_text = state.get("raw_transcript", "")
        corrected_text = state.get("current_text", "")
        video_id = state.get("video_id", "unknown")
        validation_logs = state.get("validation_logs", [])
        validated_data = state.get("validated_data", {})
        
        # Initialize manager
        manager = AutoLearningManager()
        
        # 1. Extract corrections (existing)
        corrections = manager.extract_corrections(raw_text, corrected_text, video_id)
        
        if corrections:
            print(f"   üîç Found {len(corrections)} corrections to learn")
            
            # Update knowledge bases
            stats = manager.update_knowledge_bases(corrections)
            
            total_updates = sum(stats.values())
            if total_updates > 0:
                print(f"   ‚úÖ Updated: KB={stats['kb_updates']}, Finance={stats['finance_updates']}, Errors={stats['error_updates']}")
        else:
            print(f"   ‚ÑπÔ∏è No new patterns to learn from corrections")
        
        # 2. Log market validation issues (NEW)
        if validation_logs or validated_data:
            print(f"   üìù Logging {len(validation_logs)} validation issues")
            
            # Save to asr_errors.json
            error_entries = []
            for log in validation_logs:
                error_entries.append({
                    "timestamp": datetime.now().isoformat(),
                    "video_id": video_id,
                    "error_type": "price_hallucination",
                    "description": log
                })
            
            # Add price mismatches
            for ticker, data in validated_data.items():
                if data.get("status") == "mismatch":
                    error_entries.append({
                        "timestamp": datetime.now().isoformat(),
                        "video_id": video_id,
                        "error_type": "price_hallucination",
                        "detected_stock": f"{data['original_name']} ({ticker})",
                        "asr_price": data.get("asr_price"),
                        "true_market_price": data.get("market_price"),
                        "correction_action": "replaced_with_market_price"
                    })
            
            if error_entries:
                # Append to asr_errors.json
                errors_path = manager.errors_path
                existing_errors = []
                if os.path.exists(errors_path):
                    try:
                        with open(errors_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            # Handle both list and dict formats
                            if isinstance(data, list):
                                existing_errors = data
                            elif isinstance(data, dict):
                                # Convert dict to list format
                                existing_errors = [data]
                    except:
                        pass
                
                existing_errors.extend(error_entries)
                
                with open(errors_path, 'w', encoding='utf-8') as f:
                    json.dump(existing_errors, f, ensure_ascii=False, indent=2)
                
                print(f"   ‚úÖ Logged {len(error_entries)} errors to asr_errors.json")
            
    except Exception as e:
        print(f"   ‚ö†Ô∏è Auto-learning failed (continuing): {e}")
        import traceback
        traceback.print_exc()
    
    return state  # Pass through unchanged


def targeted_correction_node(state: AgentState):
    """
    Node: ‡∏ô‡∏±‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏à‡∏∏‡∏î (Targeted Editor)

    ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ö correction_node ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Chunk ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏ô‡∏ß‡∏ô API Request ‡πÉ‡∏ô Loop ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà 2 ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡πÑ‡∏õ
    """
    print(f"\nüéØ [Node] Targeted Correction (Round {state['iteration_count'] + 1})...")
    ctx_mgr = state["data_managers"]["stock_context"]
    term_mgr = state["data_managers"]["finance_term"]
    rate_limiter = state.get("rate_limiter") # Get RateLimiter
    
    # ‡πÅ‡∏õ‡∏•‡∏á feedback ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    unknown_tickers = state.get("unknown_tickers", [])
    chunks_with_issues = state.get("chunks_with_issues", [])
    if not chunks_with_issues:
        return {"current_text": state["current_text"], "iteration_count": state["iteration_count"] + 1, "correction_suggestions": [], "chunks_with_issues": []}
    
    feedback = state["feedback_msg"]
    if unknown_tickers:
        feedback += f" ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î: {unknown_tickers}"
    if state.get("correction_suggestions"):
        feedback += "\n" + "\n".join(state["correction_suggestions"])
    
    # Targeted Correction ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏ chunk indices ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    target_indices = sorted(list(set(chunks_with_issues))) # Ensure unique and sorted indices
    print(f"   ... Targeting chunks at indices: {target_indices}")
    
    # Load learned errors
    from src.agents.llm_prompts import get_learned_errors_section
    learned_errors = get_learned_errors_section()
    
    full_corrected_text = process_large_text_robust(
        text=state["current_text"],
        chain=correction_chain,
        chunk_size=state["chunk_size"],
        rate_limiter=rate_limiter,
        extra_inputs={
            "sector_context": ctx_mgr.get_sector_prompt_str(),
            "domain_terms_context": term_mgr.get_prompt_str(),
            "learned_errors": learned_errors,
            "feedback_msg": feedback,
            "channel_name": state["channel_name"]
        }
    )

    # Step 0: Remove MANUAL [DUP] markers from LLM in targeted mode too
    try:
        from src.utils.dedup_marker_processor import remove_duplicates as remove_manual_duplicates
        full_corrected_text, manual_removed = remove_manual_duplicates(full_corrected_text)
        if manual_removed > 0:
            print(f"   üéØ Targeted Manual [DUP] markers: {manual_removed} sentences removed")
    except Exception as e:
        print(f"   ‚ö†Ô∏è Targeted manual dedup failed: {e}")
    
    return {"current_text": full_corrected_text, "iteration_count": state["iteration_count"] + 1, "correction_suggestions": [], "chunks_with_issues": []}

def fact_check_node(state: AgentState):
    """
    Node: ‡∏ô‡∏±‡∏Å‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡πÄ‡∏ó‡πá‡∏à‡∏à‡∏£‡∏¥‡∏á (Fact-Checker)

    ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞ (Proper Nouns) ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
    ‡πÄ‡∏ä‡πà‡∏ô ‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•, ‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£, ‡∏Å‡∏≠‡∏á‡∏ó‡∏∏‡∏ô, ‡∏´‡∏£‡∏∑‡∏≠‡∏®‡∏±‡∏û‡∏ó‡πå‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏ñ‡∏π‡∏Å ASR ‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏°‡∏≤‡∏ú‡∏¥‡∏î

    ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô (‡∏à‡∏≥‡∏•‡∏≠‡∏á):
    1.  ‡∏£‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ "‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å" (Unknown Tickers) ‡∏°‡∏≤‡∏à‡∏≤‡∏Å Verification Node ‡∏Ç‡∏≠‡∏á‡∏£‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
    2.  ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö Cache ‡∏ñ‡∏≤‡∏ß‡∏£ (Persistent Cache) ‡πÉ‡∏ô Resolver ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
    3.  ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô‡πÉ‡∏ô Cache ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ API ‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å
    4.  ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ô Cache ‡∏à‡∏∂‡∏á‡∏à‡∏∞‡∏ô‡∏≥‡πÑ‡∏õ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å (‡πÄ‡∏ä‡πà‡∏ô Google Search)
    5.  ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ (Correction Suggestions) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ Correction Node ‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏ñ‡∏±‡∏î‡πÑ‡∏õ

    Args:
        state (AgentState): ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏Ç‡∏≠‡∏á Graph

    Returns:
        dict: ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏î‡πâ‡∏ß‡∏¢ 'correction_suggestions' ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
    """
    print("   üîç [Node] Fact-Checking Proper Nouns (with Persistent Cache)...")
    suggestions = []
    unknowns = state.get("unknown_tickers", [])
    resolver = state["data_managers"]["resolver"] # ‡∏î‡∏∂‡∏á Resolver ‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ Cache ‡∏ñ‡∏≤‡∏ß‡∏£‡∏≠‡∏¢‡∏π‡πà‡∏†‡∏≤‡∏¢‡πÉ‡∏ô

    if not unknowns:
        return {"correction_suggestions": []}

    for unknown in unknowns:
        # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö Cache ‡∏ñ‡∏≤‡∏ß‡∏£‡πÉ‡∏ô Resolver ‡∏Å‡πà‡∏≠‡∏ô
        # ‡πÉ‡∏ä‡πâ resolver.resolve() ‡∏ã‡∏∂‡πà‡∏á‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Fuzzy Matching ‡∏Å‡∏±‡∏ö Cache ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
        cached_correction = resolver.resolve(unknown)
        
        if cached_correction:
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡πÉ‡∏ô Cache ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏à‡∏≤‡∏Å Cache ‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ‡∏ó‡∏≥‡∏Ñ‡∏≥‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
            clean_ticker = cached_correction.replace(".BK", "")
            suggestions.append(f"- ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: '{unknown}' ‡∏≠‡∏≤‡∏à‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á '{clean_ticker}' (‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å Cache ‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡πâ‡∏ß)")
            continue

        # 2. ‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏õ Search Google (‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ô Cache)
        # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏Ñ‡πâ‡∏ô Google ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏à‡∏≠
        if "macisen" in unknown.lower():
             suggestions.append("- ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: 'Macisen Seven' ‡∏≠‡∏≤‡∏à‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á 'Magnificent Seven' (‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å Web Search)")

    print(f"   ... Fact-Checker Suggestions: {suggestions}")
    return {"correction_suggestions": suggestions}

def verification_node(state: AgentState):
    """
    Node: Combined Verification (NER + Quality)
    FREE TIER MODE: Sample-based verification (1 chunk only)
    """
    print("   üîç [Node] Sample Verification (FREE TIER)...")
    resolver = state["data_managers"]["resolver"]
    current_cache = state.get("verified_entities_cache", {})

    try:
        from src.core.utils import split_text_smart
        from src.agents.llm_prompts import combined_verification_chain
        
        chunks = split_text_smart(state["current_text"], state["chunk_size"])
        
        # FREE TIER: Sample only 1 chunk (middle chunk)
        if chunks:
            sample_idx = len(chunks) // 2  # Middle chunk most representative
            sample_chunk = chunks[sample_idx]
            print(f"   ‚ö° Sampling chunk {sample_idx+1}/{len(chunks)} (saved {len(chunks)-1} requests)")
        else:
            sample_chunk = state["current_text"][:3000]
            print(f"   ‚ö° Using first 3000 chars as sample")
        
        # Single API call for both NER and quality
        result = combined_verification_chain.invoke({
            "text": sample_chunk,
            "text_sample": sample_chunk
        })
        
        entities = result.entities if hasattr(result, 'entities') else result.get("entities", [])
        quality_score = result.quality_score if hasattr(result, 'quality_score') else result.get("quality_score", 10)
        quality_reason = result.quality_reason if hasattr(result, 'quality_reason') else result.get("quality_reason", "OK")
        sample_chunk_index = sample_idx if chunks else 0
        
        print(f"   ‚úÖ Found {len(entities)} entities, Quality: {quality_score}/10")
    except Exception as e:
        print(f"   ‚ö†Ô∏è Verification error: {e}")
        entities = []
        quality_score = 6  # FREE TIER: Lower threshold
        quality_reason = "Error"
        sample_chunk_index = 0

    valid_mappings = []
    unknowns_map = {} # ‡πÉ‡∏ä‡πâ dict ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡πá‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏à‡∏≠ unknown ‡∏ó‡∏µ‡πà chunk ‡πÑ‡∏´‡∏ô
    current_cache = state.get("verified_entities_cache", {}).copy()
    seen = set()
    
    for ent in entities:
        if ent.text_found in seen: continue
        seen.add(ent.text_found)
        tk = resolver.resolve(ent.text_found)
        if tk:
            clean_tk = tk.replace(".BK", "")
            valid_mappings.append(f"- {ent.text_found} -> {clean_tk}")
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°/‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Cache ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            current_cache[ent.text_found.lower()] = clean_tk
        else:
            # TODO: Implement logic to find the chunk index where the entity was found.
            # This is a placeholder for future development.
            unknowns_map[ent.text_found] = 0 # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡πÄ‡∏à‡∏≠‡∏ó‡∏µ‡πà chunk 0
    
    unknowns = list(unknowns_map.keys())
    chunks_with_issues = list(set(unknowns_map.values()))
            
    if unknowns:
        feedback = f"‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏à‡∏≤‡∏Å Auditor: ‡∏û‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å: {unknowns}. ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç."
    elif quality_score < 8:
        feedback = f"‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏à‡∏≤‡∏Å Auditor: ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏†‡∏≤‡∏©‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏û‡∏≠ (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô {quality_score}/10). ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•: '{quality_reason}'. ‡πÇ‡∏õ‡∏£‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏´‡πâ‡∏™‡∏•‡∏∞‡∏™‡∏•‡∏ß‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô"
        if sample_chunk_index not in chunks_with_issues:
            chunks_with_issues.append(sample_chunk_index)
    else:
        feedback = "Auditor: ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß"

    print(f"   ... Auditor Feedback: {feedback}")
    return {
        "unknown_tickers": unknowns,
        "chunks_with_issues": chunks_with_issues,
        "verified_mappings": valid_mappings,
        "feedback_msg": feedback,
        "quality_score": quality_score,
        "verified_entities_cache": current_cache # ‡∏™‡πà‡∏á cache ‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏∞‡∏ö‡∏ö
    }

def summary_node(state: AgentState):
    """
    Node: ‡∏ô‡∏±‡∏Å‡∏™‡∏£‡∏∏‡∏õ (Reporter/Summarizer)

    ‡πÄ‡∏õ‡πá‡∏ô Node ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
    ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö 'current_text' ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÅ‡∏•‡πâ‡∏ß
    ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ LLM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Markdown
    
    ===== FIX: Added anti-repetition controls =====

    Args:
        state (AgentState): ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏Ç‡∏≠‡∏á Graph

    Returns:
        dict: ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏î‡πâ‡∏ß‡∏¢ 'final_summary'
    """
    print("\nüìù [Node] Generating Summary...")
    
    try:
        # ===== FIX: Use summary_chain with .bind() for parameters =====
        from src.agents.llm_prompts import summary_chain
        
        # Bind anti-repetition parameters to the LLM
        controlled_chain = summary_chain.with_config(
            configurable={
                "llm_temperature": 0.3,       # Lower randomness
                "llm_max_tokens": 3000,       # Hard limit
            }
        )
        
        # Prepare inputs
        mapping_str = "\n".join(state["verified_mappings"])
        
        # Invoke with controlled parameters
        summary = controlled_chain.invoke({
            "corrected_text": state["current_text"],
            "mapping_str": mapping_str,
            "sector_context": state["data_managers"]["stock_context"].get_sector_prompt_str(),
            "channel_name": state["channel_name"]
        })
        
        final_summary = summary if isinstance(summary, str) else str(summary)
        
        # ===== FIX: Detect bulk repetition and warn =====
        lines = [l.strip() for l in final_summary.split('\n') if l.strip()]
        if len(lines) > 0:
            from collections import Counter
            line_freq = Counter(lines)
            most_common = line_freq.most_common(1)[0]
            if most_common[1] > 10:  # Same line repeated 10+ times
                print(f"   ‚ö†Ô∏è  WARNING: Detected {most_common[1]}x repetition!")
                print(f"      Line: {most_common[0][:60]}...")
                # Apply aggressive dedup if needed
                from src.utils.safe_dedup import safe_deduplicate
                final_summary, stats = safe_deduplicate(final_summary, max_removal_percent=0.50)
                print(f"   üîß Applied aggressive dedup (removed {stats.get('removed_sentences', 0)} lines)")
        
        return {"final_summary": final_summary}
    
    except Exception as e:
        error_str = str(e)
        is_quota_error = ("429" in error_str or "RESOURCE_EXHAUSTED" in error_str)
        
        if is_quota_error:
            print("      ‚ö†Ô∏è Quota exceeded during summary generation.")
            print("      üìù Using fallback: Returning cleaned text as-is.")
            # Fallback: Return cleaned text formatted as basic markdown
            fallback_summary = f"# ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ - {state['channel_name']}\n\n"
            fallback_summary += "_‚ö†Ô∏è ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å API Quota ‡∏´‡∏°‡∏î ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß_\n\n"
            fallback_summary += "## ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n\n"
            fallback_summary += state["current_text"]
            return {"final_summary": fallback_summary}
        else:
            print(f"      ‚ùå Unexpected error during summary: {e}")
            # Generic fallback
            return {"final_summary": f"# Error\n\n‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏î‡πâ: {e}"}




def price_validation_node(state: AgentState):
    """
    Price Validation with Warning Flags (NO AUTO-CORRECTION) - ROBUST
    
    Validates stock prices against 52-week high/low and logs warnings.
    Does NOT modify prices - only flags potential issues.
    
    Features:
    - Parallel API calls for performance
    - Thread-safe logging
    - Proper error handling
    - NO auto-correction
    """
    print("\n‚ö†Ô∏è  [Price Validation] Checking prices against market data...")
    
    summary_text = state.get("final_summary", "")
    
    if not summary_text or len(summary_text) < 100:
        print("   ‚ÑπÔ∏è  Summary too short, skipping validation")
        return {}  # Return empty dict (no state mutation)
    
    try:
        from src.utils.market_validator import MarketPriceValidator
        from src.utils.price_validation_logger import PriceValidationLogger
        
        # Initialize validator and logger
        validator = MarketPriceValidator()
        logger = PriceValidationLogger()
        
        # Validate prices (parallel, robust)
        result = validator.validate_with_warnings(summary_text, max_workers=10)
        
        stats = result['stats']
        print(f"   üìä Checked: {stats['checked']} prices, Warnings: {stats['warnings']}, Errors: {stats['errors']}")
        
        if not result['has_warnings']:
            print("   ‚úÖ All prices within reasonable range (52-week ¬±10%)")
            return {}  # No warnings, return empty dict
        
        # Log warnings (thread-safe)
        video_id = state.get('video_id', 'unknown')
        video_title = state.get('video_metadata', {}).get('title', '')
        
        logger.log_warnings(
            warnings=result['warnings'],
            video_id=video_id,
            video_title=video_title
        )
        
        # Display warnings
        print(f"\n   ‚ö†Ô∏è  {len(result['warnings'])} price warnings detected:")
        for warning in result['warnings'][:3]:  # Show first 3
            print(f"      ‚Ä¢ {warning['ticker']}: {warning['message']}")
        
        if len(result['warnings']) > 3:
            print(f"      ... and {len(result['warnings']) - 3} more")
        
        # CRITICAL: Return dict of state updates, don't mutate state directly
        return {
            "price_warnings": result['warnings'],  # Store in state
            "price_validation_stats": stats
        }
        
    except Exception as e:
        print(f"   ‚ùå Price validation failed: {e}")
        import traceback
        traceback.print_exc()
        return {}  # Fail gracefully, don't crash

    """
    NEW: Market-Aware Summary Correction
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö SUMMARY ‡∏î‡πâ‡∏ß‡∏¢ yfinance ‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á output
    
    Workflow:
    1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏≤‡∏Ñ‡∏≤‡πÉ‡∏ô summary ‡∏Å‡∏±‡∏ö market data (15% tolerance)
    2. ‡∏ñ‡πâ‡∏≤‡∏ú‡∏¥‡∏î ‚Üí ‡∏™‡∏£‡πâ‡∏≤‡∏á hints (phonetic + market constraints)
    3. ‡∏Ç‡∏≠‡πÉ‡∏´‡πâ LLM ‡∏™‡∏£‡πâ‡∏≤‡∏á summary ‡πÉ‡∏´‡∏°‡πà‡∏î‡πâ‡∏ß‡∏¢ hints
    4. ‡∏≠‡∏≠‡∏Å final summary ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    """
    print("\nüíπ [Post-Summary] Market Validation & Correction...")
    
    summary_text = state.get("final_summary", "")
    
    if not summary_text or len(summary_text) < 100:
        print("   ‚ö†Ô∏è Summary too short, skipping market validation")
        return {}
    
    try:
        from src.utils.market_validator import MarketPriceValidator
        
        # Extract date from video (parse from title, not recording_date)
        video_date = None
        
        # Try to get from video_metadata
        video_title = state.get('video_metadata', {}).get('title', '')
        
        if video_title:
            # Parse date from title (e.g., "LIB Tech Talks ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 03 ‡∏ò.‡∏Ñ. 2568")
            from src.validation.fact_checker import CleanFactChecker
            checker = CleanFactChecker()
            video_date = checker.extract_date_from_filename(video_title)
        
        # Fallback to upload_date
        if not video_date:
            upload_date = state.get('video_metadata', {}).get('upload_date')
            if upload_date:
                video_date = upload_date[:10] if len(upload_date) >= 10 else upload_date
        
        # Last resort: use today (should rarely happen)
        if not video_date:
            from datetime import datetime
            video_date = datetime.now().strftime("%Y-%m-%d")
            print(f"   ‚ö†Ô∏è Could not detect video date, using today: {video_date}")
        else:
            print(f"   üìÖ Using video date: {video_date}")
        
        validator = MarketPriceValidator()
        result = validator.validate_prices(summary_text, date=video_date)
        
        print(f"   üìä Found {result['total_found']} prices, Validated: {result['validated']}/{result['total_found']}")
        
        # ‡∏ñ‡πâ‡∏≤‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‚Üí ‡∏ú‡πà‡∏≤‡∏ô
        if not result['errors']:
            print(f"   ‚úÖ All prices validated (within 15% tolerance)")
            return {}
        
        # ‡∏°‡∏µ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î ‚Üí ‡∏™‡∏£‡πâ‡∏≤‡∏á correction hints
        print(f"   ‚ö†Ô∏è {len(result['errors'])} price mismatches detected:")
        for error in result['errors'][:3]:  # Show first 3
            print(f"      {error}")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á hint message ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM (‡πÅ‡∏ö‡∏ö‡πÄ‡∏á‡∏µ‡∏¢‡∏ö - ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ö‡∏≠‡∏Å)
        hint_message = "\n\n**PRICE CORRECTION REQUIRED:**\n\n"
        
        for error in result['errors']:
            # Parse error to extract prices
            import re
            match = re.search(r'([A-Z]+).*?(\d+\.?\d*) ‡∏ö‡∏≤‡∏ó.*?(\d+\.?\d*) ‡∏ö‡∏≤‡∏ó', error)
            if match:
                ticker = match.group(1)
                wrong_price = match.group(2)
                market_price = match.group(3)
                hint_message += f"- {ticker}: ‡πÅ‡∏Å‡πâ‡∏à‡∏≤‡∏Å {wrong_price} ‡∏ö‡∏≤‡∏ó ‚Üí ‡πÄ‡∏õ‡πá‡∏ô {market_price} ‡∏ö‡∏≤‡∏ó\n"
        
        hint_message += "\n**IMPORTANT:**\n"
        hint_message += "1. ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏£‡∏≤‡∏Ñ‡∏≤‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ï‡∏•‡∏≤‡∏î (tolerance 10-15% ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ASR error)\n"
        hint_message += "2. ‡πÅ‡∏Å‡πâ‡πÄ‡∏á‡∏µ‡∏¢‡∏ö ‡πÜ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô '(Correction:...)' ‡∏´‡∏£‡∏∑‡∏≠ '(Note:...)' \n"
        hint_message += "3. summary ‡∏Ñ‡∏ß‡∏£‡∏™‡∏∞‡∏≠‡∏≤‡∏î ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢ ‡πÑ‡∏°‡πà‡∏°‡∏µ alert text\n"
        
        print(f"\n   üîÑ Re-generating summary with market hints...")
        
        # Re-run summary with hints
        from src.agents.llm_prompts import summary_chain
        
        mapping_str = "\n".join(state["verified_mappings"])
        corrected_summary_prompt = (
            f"{state['current_text']}\n\n"
            f"{hint_message}"
        )
        
        corrected_summary = summary_chain.invoke({
            "corrected_text": corrected_summary_prompt,
            "mapping_str": mapping_str,
            "sector_context": state["data_managers"]["stock_context"].get_sector_prompt_str(),
            "channel_name": state["channel_name"]
        })
        
        final_corrected = corrected_summary.content if hasattr(corrected_summary, 'content') else str(corrected_summary)
        
        # POST-PROCESS: Remove any correction notes that LLM added
        import re
        # Remove (Correction: ...) notes
        final_corrected = re.sub(r'\s*\(Correction:.*?\)', '', final_corrected, flags=re.DOTALL)
        # Remove (Note: ...) notes
        final_corrected = re.sub(r'\s*\(Note:.*?\)', '', final_corrected, flags=re.DOTALL)
        
        print(f"   ‚úÖ Market-corrected summary generated (cleaned)")
        
        return {"final_summary": final_corrected}
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è Market correction error (skipping): {e}")
        return {}  # Keep original summary



def router(state: AgentState):
    """
    Router: FREE TIER MODE
    Accept lower quality (6/10) to reduce retries
    """
    has_unknown = bool(state.get("unknown_tickers", []))
    has_suggestions = bool(state.get("correction_suggestions", []))
    quality_too_low = state.get("quality_score", 10) < 6  # FREE TIER: Lowered from 8
    
    # OPTIMIZATION: Accept unknown entities after 2 tries
    if has_unknown and state["iteration_count"] >= 2:
        print(f"   ‚ö†Ô∏è Accepting {len(state['unknown_tickers'])} unknown entities after 2 tries")
        has_unknown = False
    
    has_issues_to_fix = has_unknown or quality_too_low
    
    # Stop if: no issues OR reached max iterations
    if (not has_issues_to_fix and not has_suggestions) or state["iteration_count"] >= MAX_RETRIES:
        return "summarize"
    
    # Route to targeted correction
    return "targeted_correct" if state["iteration_count"] > 0 else "correct"

# --- Build Graph ---
def build_workflow(data_managers: Dict[str, Any]):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≠‡∏°‡πÑ‡∏û‡∏•‡πå Workflow ‡∏Ç‡∏≠‡∏á LangGraph

    ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î Node ‡πÅ‡∏•‡∏∞ Edge ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:
    - Entry Point: 'correct'
    - 'correct' -> 'auto_learning' (‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç)
    - 'auto_learning' -> 'fact_check' (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡πÄ‡∏ó‡πá‡∏à‡∏à‡∏£‡∏¥‡∏á)
    - 'fact_check' -> 'verify' (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Ticker ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û) 
    - 'verify' -> 'router' -> (‡∏ß‡∏ô‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ 'correct'/'targeted_correct' ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏õ 'summarize') 
    - 'summarize' -> END

    Args:
        data_managers (Dict[str, Any]): Dictionary ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö instance ‡∏Ç‡∏≠‡∏á Manager ‡∏ï‡πà‡∏≤‡∏á‡πÜ

    Returns:
        CompiledGraph: ‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô LangGraph ‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
    """
    
    workflow = StateGraph(AgentState)
    
    # 3-Node Validation Pipeline - DISABLED FOR ROLLBACK
    # workflow.add_node("entity_tagger", entity_tagger_node)
    # workflow.add_node("market_validator", market_validator_node)
    # workflow.add_node("content_repair", content_repair_node)
    
    # Original nodes
    workflow.add_node("correct", correction_node)
    workflow.add_node("auto_learning", auto_learning_node)
    workflow.add_node("fact_check", fact_checking_node)
    workflow.add_node("targeted_correct", targeted_correction_node)
    workflow.add_node("verify", verification_node)
    workflow.add_node("summarize", summary_node)
    workflow.add_node("price_validation", price_validation_node)  # ENABLED: Warning-only validation
    
    # ROLLBACK: Skip validation, go direct to correction
    # workflow.set_entry_point("entity_tagger")  # OLD: Started with validation
    # workflow.add_edge("entity_tagger", "market_validator")
    # workflow.add_edge("market_validator", "content_repair")
    workflow.set_entry_point("correct")  # NEW: Direct to correction
    
    # Original flow continues
    workflow.add_edge("correct", "auto_learning")
    workflow.add_edge("auto_learning", "fact_check")
    workflow.add_edge("targeted_correct", "fact_check")
    workflow.add_edge("fact_check", "verify")
    workflow.add_conditional_edges("verify", router, {
        "correct": "correct",
        "targeted_correct": "targeted_correct",
        "summarize": "summarize"
    })
    
    # Price validation after summary (warning-only)
    workflow.add_edge("summarize", "price_validation")
    workflow.add_edge("price_validation", END)
    
    # Compile with increased recursion limit
    app = workflow.compile(checkpointer=data_managers.get("checkpointer"))
    app.config = {"recursion_limit": 50}  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å default 25 ‡πÄ‡∏õ‡πá‡∏ô 50
    
    return app