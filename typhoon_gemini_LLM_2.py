import os
import io
import time
import random
import difflib
import re
import httpx
import json
from openai import OpenAI, RateLimitError, APITimeoutError
from pydub import AudioSegment
import concurrent.futures

# ---------------- CONFIGURATION ----------------

# 1. Typhoon ASR
TYPHOON_BASE_URL = "https://api.opentyphoon.ai/v1"
TYPHOON_API_KEY = ""

# 2. Google Gemini
GOOGLE_BASE_URL = "https://generativelanguage.googleapis.com/v1beta/openai/"
GOOGLE_API_KEY = "" # <--- Key ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
LLM_MODEL_NAME = "gemini-2.0-flash" 

LOCAL_AUDIO_FILE = "soundtest2/‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏∏‡πâ‡∏ô‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô_06_10_2568.mp3"

# 3. Context Biasing
INVESTMENT_PROMPT = (
    "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏∏‡πâ‡∏ô‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏à‡∏≤‡∏∞‡∏•‡∏∂‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô GUNKUL, THAICOM, Nvidia ‡πÅ‡∏•‡∏∞ Tesla "
    "‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏î‡∏±‡∏ä‡∏ô‡∏µ SET Index, Fed rate, ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à QoQ, YoY "
    "‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£ USO Phase, ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏π‡∏• TOR, Budget ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£ Ramp up ‡∏Ç‡∏≠‡∏á Yuanta"
)

# 4. SYSTEMATIC KNOWLEDGE BASE (‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ú‡∏¥‡∏î-‡∏ñ‡∏π‡∏Å)
# ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô Gemini ‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡πÅ‡∏Å‡πâ‡∏Ñ‡∏≥‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô‡∏´‡∏ô‡∏±‡∏Å‡πÜ ‡πÑ‡∏î‡πâ
DOMAIN_KNOWLEDGE = {
    "vocab_list": [
        # ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô
        {"term": "THCOM", "desc": "‡∏ö‡∏°‡∏à.‡πÑ‡∏ó‡∏¢‡∏Ñ‡∏° (Ticker)", "hints": ["‡πÑ‡∏ó‡∏¢‡∏Ñ‡∏°", "THAICOM"]},
        {"term": "GUNKUL", "desc": "‡∏ö‡∏°‡∏à.‡∏Å‡∏±‡∏ô‡∏Å‡∏∏‡∏• (Ticker)", "hints": ["‡∏Å‡∏±‡∏ô‡∏Å‡∏∏‡∏•", "‡∏Å‡∏∏‡∏ô‡∏Å‡∏∏‡∏•", "‡∏Å‡∏π‡∏ô‡∏Å‡∏∏‡∏•"]},
        
        # ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏®‡∏±‡∏û‡∏ó‡πå‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ (Jargon)
        {"term": "QoQ", "desc": "Quarter on Quarter", "hints": ["Gooncull", "‡∏à‡∏µ‡∏ß‡∏£‡∏Ñ‡∏¥‡∏ß", "‡∏Ñ‡∏¥‡∏ß‡∏Ñ‡∏¥‡∏ß"]},
        {"term": "YoY", "desc": "Year on Year", "hints": ["Y Y", "‡∏ß‡∏≤‡∏¢‡∏ß‡∏≤‡∏¢"]},
        {"term": "Assumption", "desc": "‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ê‡∏≤‡∏ô", "hints": ["Assution", "Astumption", "Apsumption"]},
        {"term": "Quarter", "desc": "‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™", "hints": ["‡∏Ñ‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå", "‡∏Ñ‡∏ß‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå", "‡∏Ñ‡∏≠‡πÄ‡∏ï‡∏≠"]},
        {"term": "TOR", "desc": "Terms of Reference", "hints": ["PR", "QR", "ER", "‡∏ó‡∏≠"]},
        {"term": "Budget", "desc": "‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì", "hints": ["‡∏ö‡∏±‡∏ï‡∏£‡πÄ‡∏à‡πá‡∏î", "‡∏ö‡∏±‡∏ï‡∏£", "‡∏°‡∏±‡∏î‡πÄ‡∏à‡πá‡∏î", "‡∏°‡∏≤‡πÄ‡∏à‡πá‡∏î"]},
        {"term": "Ramp up", "desc": "‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡πà‡∏á‡∏á‡∏≤‡∏ô", "hints": ["‡πÅ‡∏•‡∏°‡∏≤", "‡πÅ‡∏•‡∏°‡∏≤‡∏ö", "‡πÅ‡∏£‡∏°‡∏õ‡∏±‡πä‡∏û"]},
        {"term": "Reaction", "desc": "‡∏õ‡∏è‡∏¥‡∏Å‡∏¥‡∏£‡∏¥‡∏¢‡∏≤‡∏ï‡∏≠‡∏ö‡∏£‡∏±‡∏ö", "hints": ["Reax"]},
        
        # ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ñ‡∏≥‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏°‡∏±‡∏Å‡∏ú‡∏¥‡∏î (Traps)
        {"term": "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ", "desc": "‡∏Ñ‡∏≥‡∏ó‡∏±‡∏Å‡∏ó‡∏≤‡∏¢", "hints": ["‡πÇ‡∏ã‡∏î‡∏µ", "‡∏ã‡∏≠‡∏î‡∏µ"]},
        {"term": "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£", "desc": "‡∏Ñ‡∏≥‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì", "hints": ["‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå"]},
        {"term": "‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏°", "desc": "Action", "hints": ["‡∏Å‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°", "‡∏Å‡∏î‡∏ù‡∏∏‡πà‡∏ô"]},
        {"term": "‡∏ô‡πà‡∏≤‡∏à‡∏∞‡∏≠‡∏≠‡∏Å", "desc": "‡∏Ñ‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®", "hints": ["‡∏ô‡πà‡∏≤‡∏à‡∏∞‡∏ô‡∏∞", "‡∏ô‡πà‡∏≤‡∏à‡∏∞ On"]},
        
        # ‡∏Å‡∏•‡∏∏‡πà‡∏° Logic ‡∏û‡∏¥‡πÄ‡∏®‡∏©
        {"term": "‡∏ß‡πà‡∏≤ / ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà", "desc": "‡∏Ñ‡∏≥‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏° (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Valuation)", "hints": ["Valuation (‡∏ñ‡πâ‡∏≤‡∏ï‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ '‡πÑ‡∏î‡πâ‡∏á‡∏≤‡∏ô')"]},
        {"term": "USO Phase", "desc": "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏¢‡∏π‡πÇ‡∏ã", "hints": ["User Facebook", "Use Face"], 
         "logic": "‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ=Phase 2, ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏õ‡∏£‡∏∞‡∏°‡∏π‡∏•=Phase 3"},
        {"term": "Yuanta", "desc": "‡∏ö‡∏•.‡∏´‡∏¢‡∏ß‡∏ô‡∏ï‡πâ‡∏≤", "hints": ["‡∏´‡∏•‡∏≠‡∏î‡πÉ‡∏ï‡πâ", "‡∏´‡∏•‡∏ß‡∏á‡∏ï‡πâ‡∏≤", "‡∏´‡∏•‡∏≠‡∏î‡∏ï‡πâ‡∏≤", "‡πÇ‡∏´‡∏•‡∏ï‡πâ‡∏≤"]}
    ]
}

# Chunk Settings
CHUNK_DURATION_SEC = 45       
OVERLAP_DURATION_SEC = 15     
CHUNK_DURATION_MS = CHUNK_DURATION_SEC * 1000
OVERLAP_DURATION_MS = OVERLAP_DURATION_SEC * 1000

CACHE_DIR = "yt_cache"
TRANSCRIPT_OUTPUT_DIR = "transcripts_output"
TRANSCRIPT_PREFIX = "final_prod_fixed" 
MAX_WORKERS = 5

# ---------------- INIT CLIENTS ----------------

try:
    http_client = httpx.Client(timeout=120.0)
    asr_client = OpenAI(base_url=TYPHOON_BASE_URL, api_key=TYPHOON_API_KEY, http_client=http_client)
    llm_client = OpenAI(base_url=GOOGLE_BASE_URL, api_key=GOOGLE_API_KEY, http_client=http_client)
except Exception as e:
    print(f"‚ùå Error initializing Clients: {e}")
    raise SystemExit

# ---------------- CORE FUNCTIONS ----------------

def get_unique_output_path(prefix, directory, extension=".txt"):
    if not os.path.exists(directory): os.makedirs(directory)
    i = 1
    while True:
        filename = f"{prefix}_{i:02d}{extension}"
        full_path = os.path.join(directory, filename)
        if not os.path.exists(full_path): return full_path
        i += 1

def transcribe_chunk_safe(chunk_data, chunk_index, prompt, max_retries=5):
    print(f"   ‚ñ∂Ô∏è  [Chunk {chunk_index:02d}] Transcribing...")
    retries = 0
    while retries < max_retries:
        try:
            file_like = io.BytesIO(chunk_data)
            file_like.name = f"chunk_{chunk_index}.wav"
            response = asr_client.audio.transcriptions.create(
                model="typhoon-asr-realtime",
                file=file_like,
                language="th",
                prompt=prompt,
            )
            print(f"   ‚úÖ [Chunk {chunk_index:02d}] Done.")
            return response.text
        except (RateLimitError, APITimeoutError) as e:
            retries += 1
            wait_time = (2 * (2 ** retries)) + random.random()
            time.sleep(wait_time)
        except Exception as e:
            print(f"   ‚ùå [Chunk {chunk_index:02d}] Error: {e}")
            return ""
    return ""

def merge_transcriptions_fuzzy_overlap(transcripts):
    if not transcripts: return ""
    final_text = transcripts[0].strip()
    for i in range(1, len(transcripts)):
        prev_chunk = final_text
        curr_chunk = transcripts[i].strip()
        if not curr_chunk: continue
        check_len = 400 
        prev_suffix = prev_chunk[-check_len:] if len(prev_chunk) > check_len else prev_chunk
        search_range = min(len(curr_chunk), check_len)
        matcher = difflib.SequenceMatcher(None, prev_suffix, curr_chunk[:search_range])
        match = matcher.find_longest_match(0, len(prev_suffix), 0, search_range)
        if match.size > 15:
            trim_idx = match.b + match.size
            text_to_append = curr_chunk[trim_idx:]
            final_text += text_to_append
        else:
            final_text += " " + curr_chunk
    return final_text.replace("  ", " ").strip()

def clean_fillers_and_repetition(text):
    if not text: return ""
    fillers = [r"‡πÄ‡∏≠‡πà‡∏≠+", r"‡∏≠‡πà‡∏≤+", r"‡∏≠‡∏∑‡∏°+", r"‡∏≠‡πã‡∏≠+", r"‡∏≠‡∏≠+", r"‡πÅ‡∏ö‡∏ö‡∏ß‡πà‡∏≤", r"‡∏Ñ‡∏∑‡∏≠‡πÅ‡∏ö‡∏ö"]
    for filler in fillers:
        text = re.sub(filler, "", text)
    phrases = re.split(r'[\n]+', text) 
    cleaned_phrases = []
    for phrase in phrases:
        phrase = phrase.strip()
        if not phrase: continue
        is_duplicate = False
        lookback_count = 5
        start_check = max(0, len(cleaned_phrases) - lookback_count)
        for prev_phrase in cleaned_phrases[start_check:]:
            if difflib.SequenceMatcher(None, prev_phrase, phrase).ratio() > 0.85:
                is_duplicate = True
                break
        if not is_duplicate:
            cleaned_phrases.append(phrase)
    return "\n".join(cleaned_phrases)

def generate_knowledge_prompt(kb):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt ‡∏à‡∏≤‡∏Å Knowledge Base"""
    prompt = "REFERENCE KNOWLEDGE BASE (Use this mapping to fix errors):\n"
    for item in kb["vocab_list"]:
        line = f"- Correct Term: **{item['term']}**"
        if item.get("hints"):
            line += f" (Fix if sounds like: {', '.join(item['hints'])})"
        if item.get("logic"):
            line += f" [Rule: {item['logic']}]"
        prompt += line + "\n"
    return prompt

def correct_transcript_with_llm(raw_text, knowledge_base):
    print(f"\nüß† Sending to Gemini ({LLM_MODEL_NAME}) for Knowledge-Based Correction...")
    
    hallucination_triggers = ["Subtitles by", "Amara.org", "Unidentified speaker"]
    for trigger in hallucination_triggers:
        raw_text = raw_text.replace(trigger, "")
    
    raw_text = clean_fillers_and_repetition(raw_text)
    kb_prompt_str = generate_knowledge_prompt(knowledge_base)
    
    system_prompt = (
        "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ 'AI ‡∏ô‡∏±‡∏Å‡∏û‡∏¥‡∏™‡∏π‡∏à‡∏ô‡πå‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô' ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î‡πÉ‡∏ô‡∏ö‡∏ó‡∏ñ‡∏≠‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ 'Knowledge Base' ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô "
        "1. **Strict Mapping:** ‡∏´‡∏≤‡∏Å‡πÄ‡∏à‡∏≠‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ 'Fix if sounds like' ‡πÉ‡∏´‡πâ‡πÅ‡∏Å‡πâ‡πÄ‡∏õ‡πá‡∏ô 'Correct Term' ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (‡πÄ‡∏ä‡πà‡∏ô '‡∏°‡∏±‡∏î‡πÄ‡∏à‡πá‡∏î' -> 'Budget')"
        "2. **Logic Check:** ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏Å‡πâ (‡πÄ‡∏ä‡πà‡∏ô Valuation vs ‡∏ß‡πà‡∏≤, Phase 2 vs 3)"
        "3. **Verbatim:** ‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏±‡∏î‡∏ó‡∏≠‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏™‡∏≥‡∏ô‡∏ß‡∏ô"
        "4. **Format:** ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏≤, ‡πÄ‡∏ß‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢"
    )
    
    user_prompt = f"""
    ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏¥‡∏ö:
    \"\"\"{raw_text}\"\"\"
    
    ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á:
    1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î‡∏ï‡∏≤‡∏° Knowledge Base ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡πà‡∏á‡∏Ñ‡∏£‡∏±‡∏î
    2. ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô‡πÑ‡∏ó‡∏¢‡πÄ‡∏õ‡πá‡∏ô Ticker (THCOM, GUNKUL)
    3. ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ü‡∏∏‡πà‡∏°‡πÄ‡∏ü‡∏∑‡∏≠‡∏¢ (‡πÄ‡∏≠‡πà‡∏≠, ‡∏≠‡πà‡∏≤) ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô
    
    {kb_prompt_str}
    
    Output: ‡∏Ç‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß (Clean Text)
    """
    
    try:
        response = llm_client.chat.completions.create(
            model=LLM_MODEL_NAME, 
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.1, 
            max_tokens=4096 
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"‚ùå LLM Error: {e}")
        return raw_text

# ---------------- MAIN EXECUTION ----------------

def main():
    try:
        if not os.path.exists(LOCAL_AUDIO_FILE):
            print(f"‚ùå File not found: {LOCAL_AUDIO_FILE}")
            return

        print(f"\nüéß Processing: {LOCAL_AUDIO_FILE}")
        audio = AudioSegment.from_file(LOCAL_AUDIO_FILE)
        
        duration_sec = len(audio) / 1000
        print(f"** Duration: {int(duration_sec//3600):02d}:{int((duration_sec%3600)//60):02d}:{duration_sec%60:05.2f}")

        print(f"üì¶ Chunking (Chunk={CHUNK_DURATION_SEC}s, Overlap={OVERLAP_DURATION_SEC}s)...")
        chunks = []
        start = 0
        idx = 0
        while start < len(audio):
            end = min(start + CHUNK_DURATION_MS, len(audio))
            chunk = audio[start:end]
            buf = io.BytesIO()
            chunk.export(buf, format="wav")
            chunks.append({'data': buf.getvalue(), 'index': idx})
            if end == len(audio): break
            start += (CHUNK_DURATION_MS - OVERLAP_DURATION_MS) 
            idx += 1
        print(f"‚úÖ Created {len(chunks)} chunks.")

        print(f"üöÄ Starting Transcription...")
        results = {}
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_idx = {
                executor.submit(transcribe_chunk_safe, c['data'], c['index'], INVESTMENT_PROMPT): c['index'] 
                for c in chunks
            }
            for future in concurrent.futures.as_completed(future_to_idx):
                idx = future_to_idx[future]
                try:
                    results[idx] = future.result()
                except Exception:
                    results[idx] = ""

        print("üîÑ Merging text...")
        sorted_text = [results[i] for i in sorted(results.keys())]
        raw_transcript = merge_transcriptions_fuzzy_overlap(sorted_text)
        
        final_output = correct_transcript_with_llm(raw_transcript, DOMAIN_KNOWLEDGE)
        
        print("\n" + "="*40)
        print("üìÑ --- FINAL TRANSCRIPTION RESULT ---")
        print("="*40)
        print(final_output) 
        print("="*40 + "\n")
        
        out_path = get_unique_output_path(TRANSCRIPT_PREFIX, TRANSCRIPT_OUTPUT_DIR)
        with open(out_path, "w", encoding="utf-8") as f:
            f.write(final_output)
        print(f"‚úÖ Saved result to: {out_path}")

    except Exception as e:
        print(f"‚ùå Main Error: {e}")

if __name__ == "__main__":
    main()
