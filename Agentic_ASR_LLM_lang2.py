import os
import io
import time
import re
import httpx
import json
import yt_dlp
import warnings
import concurrent.futures
import difflib
import random
import logging
import argparse
import math
from datetime import datetime

from typing import List, Optional, Dict, Union
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser, StrOutputParser
from pydub import AudioSegment, effects
from tqdm import tqdm

# Load Optional Libraries
try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except ImportError:
    RecursiveCharacterTextSplitter = None
    print("‚ö†Ô∏è Warning: 'langchain-text-splitters' not installed. Fallback to simple splitting.")

try:
    from duckduckgo_search import DDGS
except ImportError:
    DDGS = None

try:
    import yfinance as yf
except ImportError:
    yf = None

try:
    from thefuzz import process
except ImportError:
    process = None
    print("‚ö†Ô∏è Warning: 'thefuzz' not installed. Fuzzy matching will be disabled.")

# -------------------------------------------------
# Configuration & Suppression
# -------------------------------------------------
warnings.filterwarnings("ignore")
logging.getLogger("httpx").setLevel(logging.ERROR)
logging.getLogger("yt_dlp").setLevel(logging.ERROR)
warnings.filterwarnings("ignore", category=RuntimeWarning, module="duckduckgo_search")

# --- API KEYS ---
TYPHOON_BASE_URL = "https://api.opentyphoon.ai/v1"
TYPHOON_API_KEY = "sk-vCE2QnUydpGnzic35kI3IcoTsAeWzb2X3jYCCAXDPmfT2JnN"  # <--- ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏≠‡∏á

GOOGLE_BASE_URL = "https://generativelanguage.googleapis.com/v1beta/openai/"
GOOGLE_API_KEY = "AIzaSyCkjWUucxzLaRPnuklKxKgYP0fUyQhTwHA"    # <--- ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏≠‡∏á
LLM_MODEL_NAME = "gemini-2.0-flash"

# --- PATHS ---
DEFAULT_YOUTUBE_URL = "https://www.youtube.com/watch?v=ks2e22C2zGA"
DOWNLOAD_DIR = "downloads"
TRANSCRIPT_OUTPUT_DIR = "transcripts_output"
MASTER_KB_FILE = "knowledge_base.json"
CACHE_FILE = "ticker_cache.json"
FINANCE_TERM_FILE = "finance_terms.json"

MAX_RETRIES = 5  # ‡πÄ‡∏û‡∏¥‡πà‡∏° Retry ‡πÉ‡∏´‡πâ‡∏ó‡∏ô‡∏ó‡∏≤‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô


# -------------------------------------------------
# Helper Functions & Dynamic Config
# -------------------------------------------------
def get_adaptive_config(duration_sec: float) -> dict:
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ Config ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ñ‡∏•‡∏¥‡∏õ (3 Tiers Strategy)
    """
    config = {}
    
    if duration_sec <= 1740: # Tier 1: 0 - 29 ‡∏ô‡∏≤‡∏ó‡∏µ
        config = {
            "CHUNK_DURATION": 45,
            "OVERLAP_DURATION": 15,
            "MAX_WORKERS": 5,
            "TEXT_CHUNK_SIZE": 4000,
            "MODE_NAME": "Tier 1: Short-Form (Fast & Agile)"
        }
    elif duration_sec <= 3600: # Tier 2: 30 - 60 ‡∏ô‡∏≤‡∏ó‡∏µ
        config = {
            "CHUNK_DURATION": 60,
            "OVERLAP_DURATION": 15,
            "MAX_WORKERS": 3,
            "TEXT_CHUNK_SIZE": 4000,
            "MODE_NAME": "Tier 2: Medium-Form (Balanced)"
        }
    else: # Tier 3: 1 ‡∏ä‡∏°. ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ (Max 2 ‡∏ä‡∏°.)
        # ‡πÉ‡∏ä‡πâ Chunk ‡πÉ‡∏´‡∏ç‡πà (10 ‡∏ô‡∏≤‡∏ó‡∏µ) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Request ‡∏•‡∏á 10 ‡πÄ‡∏ó‡πà‡∏≤ ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Rate Limit
        config = {
            "CHUNK_DURATION": 600, 
            "OVERLAP_DURATION": 20,
            "MAX_WORKERS": 2, # ‡∏•‡∏î Worker ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£
            "TEXT_CHUNK_SIZE": 3000, # ‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î Text ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô Output ‡∏ï‡∏±‡∏î‡∏à‡∏ö
            "MODE_NAME": "Tier 3: Long-Form (Stability Focus)"
        }
    return config

def sanitize_filename(name: str) -> str:
    if not name: return "audio_output"
    name = re.sub(r'[^\w\u0E00-\u0E7F\s-]', '', name)
    name = name.strip().replace(' ', '_')
    return name[:50]

def format_transcript_paragraphs(text: str) -> str:
    if not text: return text
    text = text.replace("\r\n", "\n").replace("\r", "\n")
    lines = [line.strip() for line in text.split("\n")]
    cleaned = []
    for line in lines:
        if line == "":
            if cleaned and cleaned[-1] == "": continue
            cleaned.append("")
        else:
            cleaned.append(line)
    return "\n".join(cleaned).strip()

def normalize_markdown_bullets(md: str) -> str:
    """ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô * ‡πÄ‡∏õ‡πá‡∏ô - ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢ """
    if not md: return md
    return re.sub(r"^(\s*)\* ", r"\1- ", md, flags=re.MULTILINE)

def get_file_size_mb(file_path: str) -> float:
    return os.path.getsize(file_path) / (1024 * 1024)

def format_duration(seconds: float) -> str:
    m = int(seconds // 60)
    s = int(seconds % 60)
    return f"{m}m {s}s"

def split_text_smart(text, chunk_size):
    """ ‡∏´‡∏±‡πà‡∏ô Text ‡∏ï‡∏≤‡∏°‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤ (Smart Chunking) """
    if RecursiveCharacterTextSplitter:
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=0, # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ overlap ‡πÄ‡∏≠‡∏á‡πÉ‡∏ô logic
            separators=["\n\n", "\n", " ", ""]
        )
        return splitter.split_text(text)
    else:
        return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]


# -------------------------------------------------
# PART 1: AUDIO PROCESSOR
# -------------------------------------------------
class AudioProcessor:
    @staticmethod
    def preprocess_audio(file_path: str):
        try:
            audio = AudioSegment.from_file(file_path)
            
            duration_sec = len(audio) / 1000.0
            size_mb = get_file_size_mb(file_path)
            
            print(f"\nüìä Audio Stats:")
            print(f"   - File Size:  {size_mb:.2f} MB")
            print(f"   - Duration:   {format_duration(duration_sec)}")
            
            if audio.channels > 1:
                audio = audio.set_channels(1)
            if audio.frame_rate != 16000:
                audio = audio.set_frame_rate(16000)
            audio = effects.normalize(audio)
            return audio
        except Exception as e:
            print(f"‚ùå Error processing audio: {e}")
            return None


# -------------------------------------------------
# PART 2: CONTEXT & MANAGERS
# -------------------------------------------------
class StockContextManager:
    def __init__(self, kb_file: str = MASTER_KB_FILE):
        self.kb_file = kb_file
        self.sector_data = {}
        self.all_tickers = set()
        self.search_corpus = {} 
        self.fuzzy_corpus = {}  
        self.load_kb()

    def load_kb(self):
        if not os.path.exists(self.kb_file): return
        try:
            with open(self.kb_file, "r", encoding="utf-8") as f:
                data = json.load(f)
            if isinstance(data, dict):
                self.sector_data = data
                for sector, stocks in data.items():
                    if not isinstance(stocks, dict): continue
                    for ticker, aliases in stocks.items():
                        clean_ticker = ticker.replace(".BK", "")
                        self.all_tickers.add(clean_ticker)
                        self.search_corpus[clean_ticker.lower()] = ticker
                        self.fuzzy_corpus[clean_ticker.lower()] = ticker
                        for alias in aliases:
                            self.search_corpus[alias.lower()] = ticker
                            self.fuzzy_corpus[alias.lower()] = ticker
            print(f"üìö Knowledge Base Loaded: {len(self.all_tickers)} Tickers")
        except Exception as e:
            print(f"‚ö†Ô∏è Error loading KB: {e}")

    def get_sector_prompt_str(self) -> str:
        lines = ["--- ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏° (Reference Context) ---"]
        for sector, stocks in self.sector_data.items():
            if isinstance(stocks, dict):
                tickers = [t.replace(".BK", "") for t in stocks.keys()]
                lines.append(f"## ‡∏Å‡∏•‡∏∏‡πà‡∏° {sector}:\n    - {', '.join(tickers)}")
        return "\n".join(lines)


class FinanceTermManager:
    def __init__(self, term_file: str = FINANCE_TERM_FILE):
        self.term_file = term_file
        self.data = {}
        self._load_terms()

    def _load_terms(self):
        if not os.path.exists(self.term_file): return
        try:
            with open(self.term_file, "r", encoding="utf-8") as f:
                self.data = json.load(f)
            print(f"üìò Finance Terms Loaded")
        except Exception as e:
            print(f"‚ö†Ô∏è Error loading finance_terms.json: {e}")

    def get_prompt_str(self) -> str:
        if not self.data: return "--- ‡πÑ‡∏°‡πà‡∏°‡∏µ finance terms context ---"
        lines = ["--- Domain Finance Terms & Pronunciation Patterns (Reference Only) ---"]
        for group, mapping in self.data.items():
            lines.append(f"### {group}")
            for canonical, aliases in mapping.items():
                lines.append(f"- {canonical}: {', '.join(aliases)}")
        return "\n".join(lines)


class DynamicPromptBuilder:
    def __init__(self, context_mgr: StockContextManager, term_mgr: FinanceTermManager):
        self.ctx = context_mgr
        self.term_mgr = term_mgr
        
    def build_prompt(self, metadata: dict) -> str:
        title = metadata.get("title", "")
        channel = metadata.get("channel", "")
        combined_text = f"{channel} {title}"
        candidates = re.findall(r"\b[A-Z]{2,8}\b", combined_text.upper())
        priority_tickers = [c for c in candidates if c in self.ctx.all_tickers]

        vocab_list = []
        if self.term_mgr.data:
            for group, mapping in self.term_mgr.data.items():
                vocab_list.extend(mapping.keys())
        
        selected_vocab = random.sample(vocab_list, min(len(vocab_list), 30)) if vocab_list else []

        prompt_parts = []
        if channel: prompt_parts.append(f"‡∏ä‡∏∑‡πà‡∏≠‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£: {channel}")
        if priority_tickers: prompt_parts.append(f"‡∏´‡∏∏‡πâ‡∏ô‡πÉ‡∏ô‡∏Ñ‡∏•‡∏¥‡∏õ: {', '.join(set(priority_tickers))}")
        if selected_vocab: prompt_parts.append(f"‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå: {', '.join(selected_vocab)}")
            
        return " | ".join(prompt_parts)[:1000]


# -------------------------------------------------
# PART 3: SMART RESOLVER (SAFE GUARDED)
# -------------------------------------------------
class MarketTools:
    BLACKLIST_KEYWORDS = {
        "SET", "MAI", "BKK", "THAI", "PRICE", "NEWS", 
        "STOCK", "TRADE", "DATA", "INFO", "VS", "AND",
        "TODAY", "NOW", "LIVE", "UPDATE", "INDEX"
    }

    @staticmethod
    def search_ticker(query: str) -> Optional[str]:
        if DDGS is None: return None
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                with DDGS() as ddgs:
                    results = list(ddgs.text(f"‡∏´‡∏∏‡πâ‡∏ô {query} stock symbol ticker settrade", max_results=2))
                    if not results: return None
                    blob = " ".join([r.get("title", "") + " " + r.get("body", "") for r in results]).upper()
                    
                    candidates = re.findall(r"\b([A-Z]{2,8})\.BK\b", blob)
                    if candidates: return candidates[0] + ".BK"
                    
                    candidates = re.findall(r"\b([A-Z]{2,8})\b", blob)
                    for ticker in candidates:
                        if ticker not in MarketTools.BLACKLIST_KEYWORDS and len(ticker) >= 2:
                            return ticker
        except Exception:
            return None
        return None

    @staticmethod
    def verify_ticker(ticker: str) -> bool:
        if yf is None or not ticker: return False
        if "SET.BK" in ticker or "^" in ticker: return False
        try:
            hist = yf.Ticker(ticker).history(period="1d")
            return not hist.empty
        except Exception:
            return False


class SmartMarketResolver:
    def __init__(self, context_mgr: StockContextManager, cache_file: str = CACHE_FILE):
        self.ctx = context_mgr
        self.memory = context_mgr.search_corpus.copy()
        self.cache_file = cache_file
        self.load_cache()

    def load_cache(self):
        if not os.path.exists(self.cache_file): return
        try:
            with open(self.cache_file, "r", encoding="utf-8") as f:
                self.memory.update(json.load(f))
        except: pass

    def save_cache(self, mention: str, ticker: str):
        new_data = {}
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, "r", encoding="utf-8") as f:
                    new_data = json.load(f)
            except: pass
        new_data[mention.lower()] = ticker
        with open(self.cache_file, "w", encoding="utf-8") as f:
            json.dump(new_data, f, ensure_ascii=False, indent=2)
        self.memory[mention.lower()] = ticker

    def resolve(self, mention: str) -> Optional[str]:
        mention_key = mention.lower().strip()
        mention_upper = mention.upper().strip()

        # 1. Exact Match
        if mention_key in self.memory: return self.memory[mention_key]
        if mention_upper in self.ctx.all_tickers: return mention_upper + ".BK"

        # 2. Fuzzy Match (With Safety Filters)
        if process:
            best_match, score = process.extractOne(mention_key, self.ctx.fuzzy_corpus.keys())
            ticker = self.ctx.fuzzy_corpus[best_match]
            ticker_clean = ticker.replace(".BK", "")

            # Safety Rule A: ‡∏´‡∏∏‡πâ‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏±‡πâ‡∏ô (<= 3 ‡∏ï‡∏±‡∏ß) ‡∏ï‡πâ‡∏≠‡∏á Exact Match 100%
            if len(ticker_clean) <= 3 and score < 100:
                return None

            # Safety Rule B: ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏™‡∏π‡∏á (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Bitcoin -> SSET)
            is_english = re.match(r"^[A-Za-z]+$", mention)
            if is_english and score < 95:
                return None

            if score >= 90:
                print(f"    üîç Fuzzy match: '{mention}' -> '{best_match}' ({score}%) -> {ticker}")
                self.save_cache(mention_key, ticker)
                return ticker
        
        return None


# -------------------------------------------------
# PART 4: LLM AGENTS
# -------------------------------------------------
llm = ChatOpenAI(
    base_url=GOOGLE_BASE_URL,
    api_key=GOOGLE_API_KEY,
    model=LLM_MODEL_NAME,
    temperature=0.1,
    max_tokens=8192,
)

correction_system_prompt = (
    "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ 'Senior Investment Analyst Editor' ‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ä‡πà‡∏≠‡∏á '{channel_name}'\n"
    "‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Transcript ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡πÇ‡∏î‡∏¢ **‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏ß‡πâ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏ó‡∏∏‡∏Å‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ** (Verbatim Editing)\n\n"
    "{sector_context}\n\n"
    "{domain_terms_context}\n\n"
    "--- ‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô (Strict Rules) ---\n"
    "1) **‡∏´‡πâ‡∏≤‡∏°‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏ß‡∏≤‡∏° (No Summarization):** ‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏±‡∏î‡∏ó‡∏≠‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ‡∏´‡πâ‡∏≤‡∏°‡∏£‡∏ß‡∏°‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ ‡πÉ‡∏´‡πâ‡πÅ‡∏Å‡πâ‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î ‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô ‡πÅ‡∏•‡∏∞‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n"
    "2) **Context Aware:** Input ‡∏à‡∏∞‡∏°‡∏µ [‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤: ...] ‡πÅ‡∏õ‡∏∞‡∏°‡∏≤‡∏î‡πâ‡∏ß‡∏¢ ‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à ‡πÅ‡∏ï‡πà **‡∏´‡πâ‡∏≤‡∏°‡∏û‡∏¥‡∏°‡∏û‡πå‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ô‡∏±‡πâ‡∏ô‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤** ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô Chunk ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô\n"
    "3) **Accuracy:** ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô/Crypto ‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ã‡πâ‡∏≥‡∏Ñ‡∏≥‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ\n"
    "4) **Format:** ‡πÅ‡∏ö‡πà‡∏á‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢ ‡πÅ‡∏ï‡πà‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏£‡∏ö 100%"
)
correction_prompt = ChatPromptTemplate.from_messages([
    ("system", correction_system_prompt),
    ("user", 'Transcript Chunk:\n""" {clean_text} """'),
])
correction_chain = correction_prompt | llm | StrOutputParser()

class StockEntity(BaseModel):
    text_found: str = Field(...)
class EntityList(BaseModel):
    entities: List[StockEntity]

ner_prompt = ChatPromptTemplate.from_messages([
    ("system", "‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô (Ticker), Crypto ‡∏´‡∏£‡∏∑‡∏≠‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏•‡∏á‡∏ó‡∏∏‡∏ô ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô JSON"),
    ("user", 'Text:\n""" {text} """'),
])
ner_chain = ner_prompt | llm.with_structured_output(EntityList)

summary_system_prompt = (
    "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ 'Infographic Content Lead' ‡∏Ç‡∏≠‡∏á‡∏ä‡πà‡∏≠‡∏á '{channel_name}'\n"
    "‡πÇ‡∏à‡∏ó‡∏¢‡πå: ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Markdown\n"
    "1) ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏™‡∏±‡πâ‡∏ô‡∏´‡πâ‡∏ß‡∏ô ‡∏£‡∏∞‡∏ö‡∏∏ WHAT, HOW MUCH, WHY\n"
    "2) ‡∏´‡πâ‡∏≤‡∏°‡∏ó‡∏≥‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ã‡πâ‡∏≥\n"
    "3) ‡πÉ‡∏ä‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ '-' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö bullet\n\n"
    "--- Template ---\n"
    "# ‡∏™‡∏£‡∏∏‡∏õ‡∏†‡∏≤‡∏ß‡∏∞‡∏ï‡∏•‡∏≤‡∏î\n"
    "## ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°\n"
    "## ‡∏´‡∏∏‡πâ‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô\n"
    "## ‡∏´‡∏∏‡πâ‡∏ô‡∏•‡∏á\n"
    "## ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå\n"
    "## ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ\n"
    "## ‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£\n\n"
    "[VERIFIED LIST]\n"
    "{mapping_str}"
)
summary_prompt = ChatPromptTemplate.from_messages([
    ("system", summary_system_prompt),
    ("user", 'Transcript:\n""" {corrected_text} """'),
])
summary_chain = summary_prompt | llm | StrOutputParser()


# -------------------------------------------------
# PART 5: ROBUST WORKFLOW LOGIC
# -------------------------------------------------
def process_large_text_robust(chain, full_text, chunk_size, **kwargs):
    """
    Stateful Processing with Retry Logic & Smart Chunking
    """
    chunks = split_text_smart(full_text, chunk_size)
    print(f"   ... Processing {len(chunks)} chunks sequentially ...")
    
    processed_parts = []
    previous_context = "" 
    
    for i, chunk in enumerate(chunks):
        input_args = kwargs.copy()
        
        # 1. Prepare Input with Context
        current_input_text = chunk
        if i > 0 and previous_context:
            current_input_text = f"[‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤: ...{previous_context}]\n{chunk}"
            
        if "clean_text" in input_args: input_args["clean_text"] = current_input_text
        elif "raw_text" in input_args: input_args["raw_text"] = current_input_text
        else: input_args["clean_text"] = current_input_text

        # 2. Retry Loop
        success = False
        for attempt in range(MAX_RETRIES):
            try:
                res = chain.invoke(input_args)
                
                # Clean up context echo
                if "[‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤:" in res: res = res.split("]\n")[-1].strip()
                if "[‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤" in res: res = res.split("]")[-1].strip()

                processed_parts.append(res)
                previous_context = res[-300:] 
                success = True
                break
            except Exception as e:
                wait_time = (2 ** attempt) + random.uniform(0, 1)
                print(f"      ‚ö†Ô∏è Chunk {i+1} Error (Attempt {attempt+1}/{MAX_RETRIES}): {e}. Retrying in {wait_time:.1f}s...")
                time.sleep(wait_time)
        
        if not success:
            print(f"      ‚ùå Chunk {i+1} Failed after retries. Using original text.")
            processed_parts.append(chunk)
            previous_context = chunk[-300:]

    return "\n\n".join(processed_parts)


def run_correction_loop(clean_text, ctx_mgr, term_mgr, resolver, channel_name, chunk_size):
    # [UPDATED] Use robust processing with dynamic chunk size
    corrected = process_large_text_robust(
        correction_chain, 
        clean_text, 
        chunk_size=chunk_size,
        clean_text="placeholder",
        sector_context=ctx_mgr.get_sector_prompt_str(),
        domain_terms_context=term_mgr.get_prompt_str(),
        feedback_msg="‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏û‡∏¥‡πÄ‡∏®‡∏©",
        channel_name=channel_name
    )

    try:
        ner_res = ner_chain.invoke({"text": corrected[:30000]})
        entities = ner_res.entities
    except:
        entities = []

    valid_mappings = []
    unknown_tickers = []
    seen = set()

    for ent in entities:
        if ent.text_found in seen: continue
        seen.add(ent.text_found)
        
        tk = resolver.resolve(ent.text_found)
        if tk:
            clean_tk = tk.replace(".BK", "")
            valid_mappings.append(f"- {ent.text_found} -> {clean_tk}")
        else:
            unknown_tickers.append(ent.text_found)

    if unknown_tickers:
        print(f"   ‚ö†Ô∏è Found Unknown Tickers: {unknown_tickers}")
    else:
        print("   ‚úÖ Verification Passed! All tickers found.")

    return corrected, "\n".join(valid_mappings)


# -------------------------------------------------
# PART 6: MAIN EXECUTION
# -------------------------------------------------
def transcribe_chunk(client, chunk_data, idx, prompt):
    retries = 0
    while retries < 3:
        try:
            file_like = io.BytesIO(chunk_data)
            file_like.name = f"chunk_{idx}.wav"
            response = client.audio.transcriptions.create(
                model="typhoon-asr-realtime",
                file=file_like,
                language="th",
                prompt=prompt,
                temperature=0.2,
            )
            return response.text
        except Exception as e:
            retries += 1
            time.sleep(2)
    return ""

def merge_transcriptions(transcripts: List[str]) -> str:
    if not transcripts: return ""
    full_text = transcripts[0].strip()
    for i in range(1, len(transcripts)):
        prev = full_text
        curr = transcripts[i].strip()
        if not curr: continue
        matcher = difflib.SequenceMatcher(None, prev[-500:], curr[:500])
        match = matcher.find_longest_match(0, len(prev[-500:]), 0, 500)
        if match.size > 10:
            full_text += curr[match.b + match.size :]
        else:
            full_text += " " + curr
    return re.sub(r"\s+", " ", full_text).strip()

def main(target_url: str):
    ctx_mgr = StockContextManager()
    term_mgr = FinanceTermManager()
    prompt_builder = DynamicPromptBuilder(ctx_mgr, term_mgr)
    resolver = SmartMarketResolver(ctx_mgr)

    try:
        http_client = httpx.Client(timeout=120.0)
        from openai import OpenAI
        asr_client = OpenAI(
            base_url=TYPHOON_BASE_URL,
            api_key=TYPHOON_API_KEY,
            http_client=http_client,
        )
    except Exception as e:
        print(f"‚ùå Error init Typhoon Client: {e}")
        return

    if not os.path.exists(DOWNLOAD_DIR): os.makedirs(DOWNLOAD_DIR)
    if not os.path.exists(TRANSCRIPT_OUTPUT_DIR): os.makedirs(TRANSCRIPT_OUTPUT_DIR)

    print(f"\n‚¨áÔ∏è  [Step 1] Fetching Metadata & Audio from: {target_url}")
    ydl_opts = {
        "format": "bestaudio/best",
        "postprocessors": [{"key": "FFmpegExtractAudio", "preferredcodec": "mp3", "preferredquality": "192"}],
        "outtmpl": os.path.join(DOWNLOAD_DIR, "%(id)s.%(ext)s"),
        "quiet": True,
        "no_warnings": True,
    }

    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info = ydl.extract_info(target_url, download=False)
        video_meta = {
            "title": info.get("title", "Unknown_Video"),
            "description": info.get("description", ""),
            "tags": info.get("tags", []),
            "channel": info.get("uploader", ""),
            "duration": info.get("duration", 0)
        }
        print(f"    üìÑ Title: {video_meta['title']}")
        print(f"    üì∫ Channel: {video_meta['channel']}")
        
        dynamic_prompt = prompt_builder.build_prompt(video_meta)
        print(f"    üéØ Dynamic Prompt: {dynamic_prompt}")

        # [NEW] Adaptive Configuration
        duration_sec = video_meta.get("duration", 0)
        adaptive_cfg = get_adaptive_config(duration_sec)
        
        print(f"\n‚öôÔ∏è  Adaptive Config for {format_duration(duration_sec)}:")
        print(f"    - Mode: {adaptive_cfg['MODE_NAME']}")
        print(f"    - ASR Chunk: {adaptive_cfg['CHUNK_DURATION']}s")
        print(f"    - Workers: {adaptive_cfg['MAX_WORKERS']}")
        print(f"    - Text Split: {adaptive_cfg['TEXT_CHUNK_SIZE']} chars")

        print("    ‚¨áÔ∏è  Downloading audio...")
        ydl.download([target_url])
        audio_filename = ydl.prepare_filename(info).replace(".webm", ".mp3").replace(".m4a", ".mp3")
        if not os.path.exists(audio_filename):
            audio_filename = os.path.join(DOWNLOAD_DIR, f"{info['id']}.mp3")

    # Filename Logic
    safe_title = sanitize_filename(video_meta['title'])
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    raw_filename = f"{safe_title}_RAW_{timestamp}.txt"
    clean_filename = f"{safe_title}_CLEAN_{timestamp}.txt"
    markdown_filename = f"{safe_title}_SUMMARY_{timestamp}.md"

    print("\nüîä [Step 2] Preprocessing Audio...")
    audio = AudioProcessor.preprocess_audio(audio_filename)
    if not audio: return

    print("\nüöÄ [Step 3] Transcribing with Typhoon...")
    # Use adaptive config
    chunk_ms = adaptive_cfg["CHUNK_DURATION"] * 1000
    step = chunk_ms - (adaptive_cfg["OVERLAP_DURATION"] * 1000)
    
    chunks = []
    for i, s in enumerate(range(0, len(audio), step)):
        buf = io.BytesIO()
        audio[s : min(s + chunk_ms, len(audio))].export(buf, format="wav")
        chunks.append({"data": buf.getvalue(), "index": i})

    results = {}
    with concurrent.futures.ThreadPoolExecutor(max_workers=adaptive_cfg["MAX_WORKERS"]) as ex:
        futures = {
            ex.submit(transcribe_chunk, asr_client, c["data"], c["index"], dynamic_prompt): c["index"]
            for c in chunks
        }
        for f in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc="   Processing Chunks", unit="chunk"):
            idx = futures[f]
            results[idx] = f.result()

    raw_text = merge_transcriptions([results[i] for i in sorted(results.keys())])
    
    with open(os.path.join(TRANSCRIPT_OUTPUT_DIR, raw_filename), "w", encoding="utf-8") as f:
        f.write(raw_text)

    print("\nüß† [Step 4] Intelligent Cleaning & Correction...")
    channel_name = video_meta.get("channel", "Unknown Channel")
    
    # [UPDATED] Use adaptive text chunk size
    corrected_text, mapping_str = run_correction_loop(
        raw_text, 
        ctx_mgr, 
        term_mgr, 
        resolver, 
        channel_name,
        chunk_size=adaptive_cfg["TEXT_CHUNK_SIZE"]
    )
    
    corrected_text = format_transcript_paragraphs(corrected_text)
    
    with open(os.path.join(TRANSCRIPT_OUTPUT_DIR, clean_filename), "w", encoding="utf-8") as f:
        f.write(corrected_text)

    print("\nüìù [Step 6] Generating Final Infographic Summary...")
    final_md = summary_chain.invoke({
        "corrected_text": corrected_text, 
        "mapping_str": mapping_str, 
        "channel_name": channel_name
    })
    
    final_md = normalize_markdown_bullets(final_md)

    with open(os.path.join(TRANSCRIPT_OUTPUT_DIR, markdown_filename), "w", encoding="utf-8") as f:
        f.write(final_md)

    print(f"\n‚úÖ SUCCESS! All files saved in '{TRANSCRIPT_OUTPUT_DIR}':")
    print(f"   1. {raw_filename}")
    print(f"   2. {clean_filename}")
    print(f"   3. {markdown_filename}")
    
    print("\n" + "="*20 + " OUTPUT 1: FINAL TRANSCRIPT " + "="*20 + "\n")
    # Show more context for checking
    print(corrected_text[:3000] + ("..." if len(corrected_text) > 3000 else ""))
    print("\n" + "="*22 + " END OUTPUT 1 " + "="*22 + "\n")

    print("\n" + "="*20 + " OUTPUT 2: FINAL SUMMARY " + "="*20 + "\n")
    print(final_md)
    print("\n" + "="*25 + " END OUTPUT 2 " + "=" * 25 + "\n")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Adaptive Agentic ASR Workflow")
    parser.add_argument("--url", type=str, help="Youtube URL")
    args = parser.parse_args()
    
    target_url = args.url if args.url else DEFAULT_YOUTUBE_URL
    main(target_url)